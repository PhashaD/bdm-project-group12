{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependepncies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/spark/python (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: graphframes in /opt/conda/lib/python3.11/site-packages (0.6)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from graphframes) (1.26.4)\n",
      "Requirement already satisfied: nose in /opt/conda/lib/python3.11/site-packages (from graphframes) (1.3.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "# Prepare the Spark builder with Delta extensions and set extra packages for GraphFrames\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"project3\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "# Start Spark with the extra GraphFrames package (version must match your Spark/Scala version)\n",
    "spark = configure_spark_with_delta_pip(builder, extra_packages=[\"graphframes:graphframes:0.8.4-spark3.5-s_2.12\"]).getOrCreate()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", spark._sc.defaultParallelism)\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: string (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: string (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: string (nullable = true)\n",
      " |-- DEP_TIME: string (nullable = true)\n",
      " |-- DEP_DELAY: string (nullable = true)\n",
      " |-- TAXI_OUT: string (nullable = true)\n",
      " |-- WHEELS_OFF: string (nullable = true)\n",
      " |-- WHEELS_ON: string (nullable = true)\n",
      " |-- TAXI_IN: string (nullable = true)\n",
      " |-- CRS_ARR_TIME: string (nullable = true)\n",
      " |-- ARR_TIME: string (nullable = true)\n",
      " |-- ARR_DELAY: string (nullable = true)\n",
      " |-- CANCELLED: string (nullable = true)\n",
      " |-- CANCELLATION_CODE: string (nullable = true)\n",
      " |-- DIVERTED: string (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: string (nullable = true)\n",
      " |-- ACTUAL_ELAPSED_TIME: string (nullable = true)\n",
      " |-- AIR_TIME: string (nullable = true)\n",
      " |-- DISTANCE: string (nullable = true)\n",
      " |-- CARRIER_DELAY: string (nullable = true)\n",
      " |-- WEATHER_DELAY: string (nullable = true)\n",
      " |-- NAS_DELAY: string (nullable = true)\n",
      " |-- SECURITY_DELAY: string (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: string (nullable = true)\n",
      " |-- Unnamed: 27: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"input/2009.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+------+----+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+-----------------+--------+----------------+-------------------+--------+--------+-------------+-------------+---------+--------------+-------------------+-----------+\n",
      "|   FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|DEST|CRS_DEP_TIME|DEP_TIME|DEP_DELAY|TAXI_OUT|WHEELS_OFF|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|ARR_DELAY|CANCELLED|CANCELLATION_CODE|DIVERTED|CRS_ELAPSED_TIME|ACTUAL_ELAPSED_TIME|AIR_TIME|DISTANCE|CARRIER_DELAY|WEATHER_DELAY|NAS_DELAY|SECURITY_DELAY|LATE_AIRCRAFT_DELAY|Unnamed: 27|\n",
      "+----------+----------+-----------------+------+----+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+-----------------+--------+----------------+-------------------+--------+--------+-------------+-------------+---------+--------------+-------------------+-----------+\n",
      "|2009-01-01|        XE|             1204|   DCA| EWR|        1100|  1058.0|     -2.0|    18.0|    1116.0|   1158.0|    8.0|        1202|  1206.0|      4.0|      0.0|             NULL|     0.0|            62.0|               68.0|    42.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1206|   EWR| IAD|        1510|  1509.0|     -1.0|    28.0|    1537.0|   1620.0|    4.0|        1632|  1624.0|     -8.0|      0.0|             NULL|     0.0|            82.0|               75.0|    43.0|   213.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1207|   EWR| DCA|        1100|  1059.0|     -1.0|    20.0|    1119.0|   1155.0|    6.0|        1210|  1201.0|     -9.0|      0.0|             NULL|     0.0|            70.0|               62.0|    36.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1208|   DCA| EWR|        1240|  1249.0|      9.0|    10.0|    1259.0|   1336.0|    9.0|        1357|  1345.0|    -12.0|      0.0|             NULL|     0.0|            77.0|               56.0|    37.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1209|   IAD| EWR|        1715|  1705.0|    -10.0|    24.0|    1729.0|   1809.0|   13.0|        1900|  1822.0|    -38.0|      0.0|             NULL|     0.0|           105.0|               77.0|    40.0|   213.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1212|   ATL| EWR|        1915|  1913.0|     -2.0|    19.0|    1932.0|   2108.0|   15.0|        2142|  2123.0|    -19.0|      0.0|             NULL|     0.0|           147.0|              130.0|    96.0|   745.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1212|   CLE| ATL|        1645|  1637.0|     -8.0|    12.0|    1649.0|   1820.0|    5.0|        1842|  1825.0|    -17.0|      0.0|             NULL|     0.0|           117.0|              108.0|    91.0|   554.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1214|   DCA| EWR|        1915|  1908.0|     -7.0|     9.0|    1917.0|   1953.0|   34.0|        2035|  2027.0|     -8.0|      0.0|             NULL|     0.0|            80.0|               79.0|    36.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1215|   EWR| DCA|        1715|  1710.0|     -5.0|    28.0|    1738.0|   1819.0|    4.0|        1838|  1823.0|    -15.0|      0.0|             NULL|     0.0|            83.0|               73.0|    41.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1217|   EWR| DCA|        1300|  1255.0|     -5.0|    15.0|    1310.0|   1349.0|    7.0|        1408|  1356.0|    -12.0|      0.0|             NULL|     0.0|            68.0|               61.0|    39.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1218|   DCA| EWR|        1500|  1457.0|     -3.0|    14.0|    1511.0|   1552.0|    7.0|        1620|  1559.0|    -21.0|      0.0|             NULL|     0.0|            80.0|               62.0|    41.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1219|   EWR| DCA|        2135|  2131.0|     -4.0|    21.0|    2152.0|   2232.0|    3.0|        2252|  2235.0|    -17.0|      0.0|             NULL|     0.0|            77.0|               64.0|    40.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1220|   CLE| DCA|        1905|  1855.0|    -10.0|    10.0|    1905.0|   1956.0|    5.0|        2025|  2001.0|    -24.0|      0.0|             NULL|     0.0|            80.0|               66.0|    51.0|   310.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1220|   DCA| EWR|        2100|  2049.0|    -11.0|    10.0|    2059.0|   2133.0|   10.0|        2217|  2143.0|    -34.0|      0.0|             NULL|     0.0|            77.0|               54.0|    34.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1232|   ORD| EWR|         905|   900.0|     -5.0|    16.0|     916.0|   1144.0|    6.0|        1212|  1150.0|    -22.0|      0.0|             NULL|     0.0|           127.0|              110.0|    88.0|   719.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1233|   EWR| ORD|        1000|  1035.0|     35.0|    14.0|    1049.0|   1156.0|   10.0|        1139|  1206.0|     27.0|      0.0|             NULL|     0.0|           159.0|              151.0|   127.0|   719.0|          0.0|         27.0|      0.0|           0.0|                0.0|       NULL|\n",
      "|2009-01-01|        XE|             1234|   ORD| EWR|        1230|  1234.0|      4.0|     8.0|    1242.0|   1511.0|   14.0|        1559|  1525.0|    -34.0|      0.0|             NULL|     0.0|           149.0|              111.0|    89.0|   719.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1235|   EWR| ORD|        1343|  1406.0|     23.0|    13.0|    1419.0|   1523.0|    7.0|        1523|  1530.0|      7.0|      0.0|             NULL|     0.0|           160.0|              144.0|   124.0|   719.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1236|   ORD| EWR|        1630|  1619.0|    -11.0|    19.0|    1638.0|   1906.0|   35.0|        2002|  1941.0|    -21.0|      0.0|             NULL|     0.0|           152.0|              142.0|    88.0|   719.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1237|   EWR| ORD|        1930|  1927.0|     -3.0|    16.0|    1943.0|   2049.0|    8.0|        2123|  2057.0|    -26.0|      0.0|             NULL|     0.0|           173.0|              150.0|   126.0|   719.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "+----------+----------+-----------------+------+----+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+-----------------+--------+----------------+-------------------+--------+--------+-------------+-------------+---------+--------------+-------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Cast CANCELLED to float\n",
    "df = df.withColumn(\"CANCELLED\", col(\"CANCELLED\").cast(\"float\"))\n",
    "\n",
    "# Filter on 0.0\n",
    "df_clean = (\n",
    "    df\n",
    "    .filter(col(\"CANCELLED\") == 0.0)\n",
    "    .dropna(subset=[\"ORIGIN\", \"DEST\"])\n",
    "    .filter(col(\"ORIGIN\") != col(\"DEST\"))\n",
    ")\n",
    "df_clean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th></tr>\n",
       "<tr><td>DCA</td></tr>\n",
       "<tr><td>ABQ</td></tr>\n",
       "<tr><td>LBB</td></tr>\n",
       "<tr><td>PVD</td></tr>\n",
       "<tr><td>AVL</td></tr>\n",
       "<tr><td>DSM</td></tr>\n",
       "<tr><td>XNA</td></tr>\n",
       "<tr><td>SYR</td></tr>\n",
       "<tr><td>CAE</td></tr>\n",
       "<tr><td>FAT</td></tr>\n",
       "<tr><td>MBS</td></tr>\n",
       "<tr><td>ROA</td></tr>\n",
       "<tr><td>SAN</td></tr>\n",
       "<tr><td>AZO</td></tr>\n",
       "<tr><td>LNK</td></tr>\n",
       "<tr><td>HPN</td></tr>\n",
       "<tr><td>ANC</td></tr>\n",
       "<tr><td>MYR</td></tr>\n",
       "<tr><td>TRI</td></tr>\n",
       "<tr><td>TOL</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+---+\n",
       "| id|\n",
       "+---+\n",
       "|DCA|\n",
       "|ABQ|\n",
       "|LBB|\n",
       "|PVD|\n",
       "|AVL|\n",
       "|DSM|\n",
       "|XNA|\n",
       "|SYR|\n",
       "|CAE|\n",
       "|FAT|\n",
       "|MBS|\n",
       "|ROA|\n",
       "|SAN|\n",
       "|AZO|\n",
       "|LNK|\n",
       "|HPN|\n",
       "|ANC|\n",
       "|MYR|\n",
       "|TRI|\n",
       "|TOL|\n",
       "+---+\n",
       "only showing top 20 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from graphframes import GraphFrame\n",
    "\n",
    "vertices = df_clean.select(col(\"ORIGIN\").alias(\"id\")) \\\n",
    "    .union(df_clean.select(col(\"DEST\").alias(\"id\"))) \\\n",
    "    .distinct()\n",
    "\n",
    "display(vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>src</th><th>dst</th></tr>\n",
       "<tr><td>DCA</td><td>EWR</td></tr>\n",
       "<tr><td>EWR</td><td>IAD</td></tr>\n",
       "<tr><td>EWR</td><td>DCA</td></tr>\n",
       "<tr><td>DCA</td><td>EWR</td></tr>\n",
       "<tr><td>IAD</td><td>EWR</td></tr>\n",
       "<tr><td>ATL</td><td>EWR</td></tr>\n",
       "<tr><td>CLE</td><td>ATL</td></tr>\n",
       "<tr><td>DCA</td><td>EWR</td></tr>\n",
       "<tr><td>EWR</td><td>DCA</td></tr>\n",
       "<tr><td>EWR</td><td>DCA</td></tr>\n",
       "<tr><td>DCA</td><td>EWR</td></tr>\n",
       "<tr><td>EWR</td><td>DCA</td></tr>\n",
       "<tr><td>CLE</td><td>DCA</td></tr>\n",
       "<tr><td>DCA</td><td>EWR</td></tr>\n",
       "<tr><td>ORD</td><td>EWR</td></tr>\n",
       "<tr><td>EWR</td><td>ORD</td></tr>\n",
       "<tr><td>ORD</td><td>EWR</td></tr>\n",
       "<tr><td>EWR</td><td>ORD</td></tr>\n",
       "<tr><td>ORD</td><td>EWR</td></tr>\n",
       "<tr><td>EWR</td><td>ORD</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+---+---+\n",
       "|src|dst|\n",
       "+---+---+\n",
       "|DCA|EWR|\n",
       "|EWR|IAD|\n",
       "|EWR|DCA|\n",
       "|DCA|EWR|\n",
       "|IAD|EWR|\n",
       "|ATL|EWR|\n",
       "|CLE|ATL|\n",
       "|DCA|EWR|\n",
       "|EWR|DCA|\n",
       "|EWR|DCA|\n",
       "|DCA|EWR|\n",
       "|EWR|DCA|\n",
       "|CLE|DCA|\n",
       "|DCA|EWR|\n",
       "|ORD|EWR|\n",
       "|EWR|ORD|\n",
       "|ORD|EWR|\n",
       "|EWR|ORD|\n",
       "|ORD|EWR|\n",
       "|EWR|ORD|\n",
       "+---+---+\n",
       "only showing top 20 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "edges = df_clean.select(\n",
    "    col(\"ORIGIN\").alias(\"src\"),\n",
    "    col(\"DEST\").alias(\"dst\")\n",
    ")\n",
    "\n",
    "display(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphFrame(v:[id: string], e:[src: string, dst: string])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = GraphFrame(vertices, edges)\n",
    "\n",
    "vertices.cache()\n",
    "edges.cache()\n",
    "\n",
    "display(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 1: Compute different statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>in_degree</th><th>out_degree</th><th>total_degree</th></tr>\n",
       "<tr><td>DCA</td><td>78016</td><td>77900</td><td>155916</td></tr>\n",
       "<tr><td>ABQ</td><td>35419</td><td>35393</td><td>70812</td></tr>\n",
       "<tr><td>LBB</td><td>7891</td><td>7873</td><td>15764</td></tr>\n",
       "<tr><td>PVD</td><td>18112</td><td>18090</td><td>36202</td></tr>\n",
       "<tr><td>AVL</td><td>4493</td><td>4470</td><td>8963</td></tr>\n",
       "<tr><td>DSM</td><td>14787</td><td>14729</td><td>29516</td></tr>\n",
       "<tr><td>XNA</td><td>13409</td><td>13343</td><td>26752</td></tr>\n",
       "<tr><td>SYR</td><td>9207</td><td>9189</td><td>18396</td></tr>\n",
       "<tr><td>CAE</td><td>9803</td><td>9796</td><td>19599</td></tr>\n",
       "<tr><td>FAT</td><td>12255</td><td>12231</td><td>24486</td></tr>\n",
       "<tr><td>MBS</td><td>3372</td><td>3352</td><td>6724</td></tr>\n",
       "<tr><td>ROA</td><td>3630</td><td>3602</td><td>7232</td></tr>\n",
       "<tr><td>SAN</td><td>82109</td><td>82013</td><td>164122</td></tr>\n",
       "<tr><td>AZO</td><td>2839</td><td>2835</td><td>5674</td></tr>\n",
       "<tr><td>LNK</td><td>2732</td><td>2719</td><td>5451</td></tr>\n",
       "<tr><td>HPN</td><td>10489</td><td>10454</td><td>20943</td></tr>\n",
       "<tr><td>ANC</td><td>17367</td><td>17434</td><td>34801</td></tr>\n",
       "<tr><td>MYR</td><td>4446</td><td>4423</td><td>8869</td></tr>\n",
       "<tr><td>TRI</td><td>2539</td><td>2531</td><td>5070</td></tr>\n",
       "<tr><td>TOL</td><td>1322</td><td>1318</td><td>2640</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+---+---------+----------+------------+\n",
       "| id|in_degree|out_degree|total_degree|\n",
       "+---+---------+----------+------------+\n",
       "|DCA|    78016|     77900|      155916|\n",
       "|ABQ|    35419|     35393|       70812|\n",
       "|LBB|     7891|      7873|       15764|\n",
       "|PVD|    18112|     18090|       36202|\n",
       "|AVL|     4493|      4470|        8963|\n",
       "|DSM|    14787|     14729|       29516|\n",
       "|XNA|    13409|     13343|       26752|\n",
       "|SYR|     9207|      9189|       18396|\n",
       "|CAE|     9803|      9796|       19599|\n",
       "|FAT|    12255|     12231|       24486|\n",
       "|MBS|     3372|      3352|        6724|\n",
       "|ROA|     3630|      3602|        7232|\n",
       "|SAN|    82109|     82013|      164122|\n",
       "|AZO|     2839|      2835|        5674|\n",
       "|LNK|     2732|      2719|        5451|\n",
       "|HPN|    10489|     10454|       20943|\n",
       "|ANC|    17367|     17434|       34801|\n",
       "|MYR|     4446|      4423|        8869|\n",
       "|TRI|     2539|      2531|        5070|\n",
       "|TOL|     1322|      1318|        2640|\n",
       "+---+---------+----------+------------+\n",
       "only showing top 20 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Compute out-degree: count flights leaving each airport.\n",
    "out_degree = edges.groupBy(\"src\").agg(count(\"*\").alias(\"out_degree\")) \\\n",
    "                  .withColumnRenamed(\"src\", \"id\")\n",
    "\n",
    "# Compute in-degree: count flights arriving at each airport.\n",
    "in_degree = edges.groupBy(\"dst\").agg(count(\"*\").alias(\"in_degree\")) \\\n",
    "                 .withColumnRenamed(\"dst\", \"id\")\n",
    "\n",
    "# Combine the results with vertices to include all airports,\n",
    "degree_df = vertices.join(in_degree, on=\"id\", how=\"left\") \\\n",
    "                    .join(out_degree, on=\"id\", how=\"left\") \\\n",
    "                    .na.fill(0) \\\n",
    "                    .withColumn(\"total_degree\", col(\"in_degree\") + col(\"out_degree\"))\n",
    "\n",
    "# Display the degree statistics\n",
    "display(degree_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triangle count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|vertex|triangleCount|\n",
      "+------+-------------+\n",
      "|   AVL|         10.0|\n",
      "|   CAE|         17.5|\n",
      "|   SAN|          8.0|\n",
      "|   ABQ|        155.5|\n",
      "|   DCA|        205.5|\n",
      "|   AZO|          3.0|\n",
      "|   DSM|         23.5|\n",
      "|   ANC|         61.0|\n",
      "|   FAT|          6.0|\n",
      "|   LNK|          2.5|\n",
      "|   HPN|          4.0|\n",
      "|   MBS|          0.5|\n",
      "|   BGM|          0.0|\n",
      "|   LBB|          0.0|\n",
      "|   CDV|          0.0|\n",
      "|   BJI|          0.0|\n",
      "|   DTW|        347.5|\n",
      "|   LGA|         63.5|\n",
      "|   BOI|         38.5|\n",
      "|   AUS|        193.0|\n",
      "+------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Calculate triangle count (undirected):\n",
    "def triangleCount(graph):\n",
    "    edges = graph.edges.select(F.least(\"src\", \"dst\").alias(\"src\"), F.greatest(\"src\", \"dst\").alias(\"dst\")).distinct()\n",
    "\n",
    "    # 1. Find neighbors for each node.\n",
    "    neighbors = edges.groupBy(\"src\").agg(F.collect_set(\"dst\").alias(\"neighbors\"))\n",
    "\n",
    "    # 2. Join neighbors with edges to find pairs of neighbors for a node.\n",
    "    neighbor_pairs = neighbors.alias(\"n1\").join(edges.alias(\"e\"), F.col(\"n1.src\") == F.col(\"e.src\")) \\\n",
    "        .join(neighbors.alias(\"n2\"), F.col(\"e.dst\") == F.col(\"n2.src\")) \\\n",
    "        .select(F.col(\"n1.src\").alias(\"node1\"), F.col(\"n1.neighbors\").alias(\"neighbors1\"),\n",
    "                F.col(\"n2.src\").alias(\"node2\"), F.col(\"n2.neighbors\").alias(\"neighbors2\"))\n",
    "\n",
    "    # 3. Find intersections of neighbor sets and count.\n",
    "    triangles = neighbor_pairs.withColumn(\"intersection_size\", F.size(F.array_intersect(\"neighbors1\", \"neighbors2\"))) \\\n",
    "        .select(\"node1\", \"node2\", \"intersection_size\")\n",
    "\n",
    "    # 4. Group by node1 to calculate total triangles.\n",
    "    triangle_counts = triangles.groupBy(\"node1\").agg(F.sum(\"intersection_size\").alias(\"total_triangles\")) \\\n",
    "        .withColumn(\"triangleCount\", F.expr(\"total_triangles / 2\")) \\\n",
    "        .select(F.col(\"node1\").alias(\"vertex\"), \"triangleCount\")\n",
    "\n",
    "    return triangle_counts\n",
    "\n",
    "triangle_counts = triangleCount(graph)\n",
    "triangle_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 2: Total number of triangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of triangles: 7995.5\n"
     ]
    }
   ],
   "source": [
    "total_triangle_count = triangle_counts.agg(F.sum(\"triangleCount\").alias(\"total\")).collect()[0][\"total\"]\n",
    "print(\"Total number of triangles:\", total_triangle_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 3: Closeness Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, col, when\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def compute_closeness_centrality(vertices, edges, max_depth=10):\n",
    "    closeness_scores = []\n",
    "\n",
    "    vertex_ids = [row[\"id\"] for row in vertices.select(\"id\").collect()]\n",
    "\n",
    "    for source in vertex_ids:\n",
    "        print(f\"Processing: {source}\")  # debug log\n",
    "\n",
    "        visited = vertices.withColumn(\"distance\", when(col(\"id\") == source, lit(0)))\n",
    "        frontier = visited.filter(col(\"id\") == source)\n",
    "\n",
    "        depth = 0\n",
    "        while frontier.count() > 0 and depth < max_depth:\n",
    "            print(f\"Depth: {depth}, Frontier size: {frontier.count()}\")  # debug log\n",
    "\n",
    "            next_frontier = frontier.join(edges, frontier[\"id\"] == edges[\"src\"]) \\\n",
    "                                    .select(edges[\"dst\"].alias(\"id\")) \\\n",
    "                                    .distinct()\n",
    "\n",
    "            next_frontier = next_frontier.join(visited, \"id\", \"left_anti\") \\\n",
    "                                         .withColumn(\"distance\", lit(depth + 1))\n",
    "\n",
    "            visited = visited.union(next_frontier).distinct()\n",
    "            frontier = next_frontier\n",
    "            depth += 1\n",
    "\n",
    "        reachable = visited.filter((col(\"id\") != source) & col(\"distance\").isNotNull())\n",
    "        reachable_count = reachable.count()\n",
    "\n",
    "        if reachable_count > 0:\n",
    "            total_distance = reachable.agg({\"distance\": \"sum\"}).collect()[0][0]\n",
    "            avg_distance = total_distance / reachable_count\n",
    "            closeness = 1 / avg_distance if avg_distance > 0 else 0\n",
    "        else:\n",
    "            closeness = 0\n",
    "\n",
    "        closeness_scores.append((source, closeness))\n",
    "\n",
    "    return spark.createDataFrame(closeness_scores, [\"id\", \"closeness_centrality\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import gc\n",
    "import time\n",
    "\n",
    "def compute_all_closeness_in_batches(vertices: DataFrame, edges: DataFrame, batch_size: int = 10, max_depth: int = 3) -> DataFrame:\n",
    "    from pyspark.sql.functions import col\n",
    "\n",
    "    airport_ids = [row[\"id\"] for row in vertices.collect()]\n",
    "    total = len(airport_ids)\n",
    "    all_results = []\n",
    "\n",
    "    for i in range(0, total, batch_size):\n",
    "        print(f\"\\n🧮 Batch {i // batch_size + 1}: Processing airports {i} to {min(i + batch_size, total)}...\")\n",
    "\n",
    "        batch_ids = airport_ids[i:i + batch_size]\n",
    "        batch_vertices = vertices.filter(col(\"id\").isin(batch_ids))\n",
    "\n",
    "        # Try/catch for robustness\n",
    "        try:\n",
    "            batch_result = compute_closeness_centrality(batch_vertices, edges, max_depth=max_depth)\n",
    "            all_results.append(batch_result)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in batch {i // batch_size + 1}: {e}\")\n",
    "        \n",
    "        # Give Spark a break (crucial in Docker)\n",
    "        spark.catalog.clearCache()\n",
    "        gc.collect()\n",
    "        time.sleep(2)\n",
    "\n",
    "    # Combine results safely\n",
    "    if not all_results:\n",
    "        return spark.createDataFrame([], schema=\"id STRING, closeness_centrality DOUBLE\")\n",
    "\n",
    "    combined = all_results[0]\n",
    "    for df in all_results[1:]:\n",
    "        combined = combined.union(df)\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧮 Batch 1: Processing airports 0 to 5...\n",
      "Processing: DCA\n",
      "Depth: 0, Frontier size: 1\n",
      "Depth: 1, Frontier size: 47\n",
      "Processing: ABQ\n",
      "Depth: 0, Frontier size: 1\n",
      "Depth: 1, Frontier size: 30\n",
      "Processing: LBB\n",
      "Depth: 0, Frontier size: 1\n",
      "Depth: 1, Frontier size: 7\n",
      "Processing: PVD\n",
      "Depth: 0, Frontier size: 1\n",
      "Depth: 1, Frontier size: 19\n",
      "Processing: AVL\n",
      "Depth: 0, Frontier size: 1\n",
      "Depth: 1, Frontier size: 8\n",
      "\n",
      "🧮 Batch 2: Processing airports 5 to 10...\n",
      "Processing: DSM\n",
      "Depth: 0, Frontier size: 1\n",
      "Depth: 1, Frontier size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error in batch 2: An error occurred while calling o939.count\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m closeness_df_batched \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_all_closeness_in_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvertices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m display(closeness_df_batched\u001b[38;5;241m.\u001b[39morderBy(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcloseness_centrality\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdesc()))\n",
      "Cell \u001b[0;32mIn[9], line 28\u001b[0m, in \u001b[0;36mcompute_all_closeness_in_batches\u001b[0;34m(vertices, edges, batch_size, max_depth)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ Error in batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mbatch_size\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Give Spark a break (crucial in Docker)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcatalog\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclearCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     30\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/catalog.py:1099\u001b[0m, in \u001b[0;36mCatalog.clearCache\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclearCache\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Removes all cached tables from the in-memory cache.\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m \n\u001b[1;32m   1088\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;124;03m    >>> _ = spark.sql(\"DROP TABLE tbl1\")\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1099\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcatalog\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclearCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "closeness_df_batched = compute_all_closeness_in_batches(vertices.limit(5), edges, batch_size=5, max_depth=2)\n",
    "display(closeness_df_batched.orderBy(col(\"closeness_centrality\").desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total edges: 6342300\n",
      "+---+---+\n",
      "|src|dst|\n",
      "+---+---+\n",
      "|DCA|EWR|\n",
      "|EWR|IAD|\n",
      "|EWR|DCA|\n",
      "|DCA|EWR|\n",
      "|IAD|EWR|\n",
      "+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Total edges:\", edges.count())\n",
    "edges.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges with unmatched nodes: 0\n",
      "+---+---+\n",
      "|src|dst|\n",
      "+---+---+\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length, trim\n",
    "\n",
    "# Check for edge values not in vertices\n",
    "edges_not_in_vertices = edges.filter(~col(\"src\").isin([row[\"id\"] for row in vertices.collect()]) |\n",
    "                                     ~col(\"dst\").isin([row[\"id\"] for row in vertices.collect()]))\n",
    "\n",
    "print(\"Edges with unmatched nodes:\", edges_not_in_vertices.count())\n",
    "edges_not_in_vertices.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From ATL, reachable nodes within depth 3: 296\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "airport = \"ATL\"  # a well-connected airport\n",
    "\n",
    "frontier = deque([airport])\n",
    "visited = set()\n",
    "depth = 0\n",
    "max_depth = 3\n",
    "\n",
    "while frontier and depth < max_depth:\n",
    "    next_level = deque()\n",
    "    for node in frontier:\n",
    "        neighbors = edges.filter(edges[\"src\"] == node).select(\"dst\").rdd.flatMap(lambda x: x).collect()\n",
    "        for n in neighbors:\n",
    "            if n not in visited:\n",
    "                next_level.append(n)\n",
    "                visited.add(n)\n",
    "    frontier = next_level\n",
    "    depth += 1\n",
    "\n",
    "print(f\"From {airport}, reachable nodes within depth {max_depth}: {len(visited)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mcount\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'count' is not defined"
     ]
    }
   ],
   "source": [
    "del count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
