{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependepncies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/spark/python (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: graphframes in /opt/conda/lib/python3.11/site-packages (0.6)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from graphframes) (1.26.4)\n",
      "Requirement already satisfied: nose in /opt/conda/lib/python3.11/site-packages (from graphframes) (1.3.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "# Prepare the Spark builder with Delta extensions and set extra packages for GraphFrames\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"project3\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "# Start Spark with the extra GraphFrames package (version must match your Spark/Scala version)\n",
    "spark = configure_spark_with_delta_pip(builder, extra_packages=[\"graphframes:graphframes:0.8.4-spark3.5-s_2.12\"]).getOrCreate()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", spark._sc.defaultParallelism)\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: string (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: string (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: string (nullable = true)\n",
      " |-- DEP_TIME: string (nullable = true)\n",
      " |-- DEP_DELAY: string (nullable = true)\n",
      " |-- TAXI_OUT: string (nullable = true)\n",
      " |-- WHEELS_OFF: string (nullable = true)\n",
      " |-- WHEELS_ON: string (nullable = true)\n",
      " |-- TAXI_IN: string (nullable = true)\n",
      " |-- CRS_ARR_TIME: string (nullable = true)\n",
      " |-- ARR_TIME: string (nullable = true)\n",
      " |-- ARR_DELAY: string (nullable = true)\n",
      " |-- CANCELLED: string (nullable = true)\n",
      " |-- CANCELLATION_CODE: string (nullable = true)\n",
      " |-- DIVERTED: string (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: string (nullable = true)\n",
      " |-- ACTUAL_ELAPSED_TIME: string (nullable = true)\n",
      " |-- AIR_TIME: string (nullable = true)\n",
      " |-- DISTANCE: string (nullable = true)\n",
      " |-- CARRIER_DELAY: string (nullable = true)\n",
      " |-- WEATHER_DELAY: string (nullable = true)\n",
      " |-- NAS_DELAY: string (nullable = true)\n",
      " |-- SECURITY_DELAY: string (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: string (nullable = true)\n",
      " |-- Unnamed: 27: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"input/2009.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+------+----+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+-----------------+--------+----------------+-------------------+--------+--------+-------------+-------------+---------+--------------+-------------------+-----------+\n",
      "|   FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|DEST|CRS_DEP_TIME|DEP_TIME|DEP_DELAY|TAXI_OUT|WHEELS_OFF|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|ARR_DELAY|CANCELLED|CANCELLATION_CODE|DIVERTED|CRS_ELAPSED_TIME|ACTUAL_ELAPSED_TIME|AIR_TIME|DISTANCE|CARRIER_DELAY|WEATHER_DELAY|NAS_DELAY|SECURITY_DELAY|LATE_AIRCRAFT_DELAY|Unnamed: 27|\n",
      "+----------+----------+-----------------+------+----+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+-----------------+--------+----------------+-------------------+--------+--------+-------------+-------------+---------+--------------+-------------------+-----------+\n",
      "|2009-01-01|        XE|             1204|   DCA| EWR|        1100|  1058.0|     -2.0|    18.0|    1116.0|   1158.0|    8.0|        1202|  1206.0|      4.0|      0.0|             NULL|     0.0|            62.0|               68.0|    42.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1206|   EWR| IAD|        1510|  1509.0|     -1.0|    28.0|    1537.0|   1620.0|    4.0|        1632|  1624.0|     -8.0|      0.0|             NULL|     0.0|            82.0|               75.0|    43.0|   213.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1207|   EWR| DCA|        1100|  1059.0|     -1.0|    20.0|    1119.0|   1155.0|    6.0|        1210|  1201.0|     -9.0|      0.0|             NULL|     0.0|            70.0|               62.0|    36.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1208|   DCA| EWR|        1240|  1249.0|      9.0|    10.0|    1259.0|   1336.0|    9.0|        1357|  1345.0|    -12.0|      0.0|             NULL|     0.0|            77.0|               56.0|    37.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1209|   IAD| EWR|        1715|  1705.0|    -10.0|    24.0|    1729.0|   1809.0|   13.0|        1900|  1822.0|    -38.0|      0.0|             NULL|     0.0|           105.0|               77.0|    40.0|   213.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1212|   ATL| EWR|        1915|  1913.0|     -2.0|    19.0|    1932.0|   2108.0|   15.0|        2142|  2123.0|    -19.0|      0.0|             NULL|     0.0|           147.0|              130.0|    96.0|   745.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1212|   CLE| ATL|        1645|  1637.0|     -8.0|    12.0|    1649.0|   1820.0|    5.0|        1842|  1825.0|    -17.0|      0.0|             NULL|     0.0|           117.0|              108.0|    91.0|   554.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1214|   DCA| EWR|        1915|  1908.0|     -7.0|     9.0|    1917.0|   1953.0|   34.0|        2035|  2027.0|     -8.0|      0.0|             NULL|     0.0|            80.0|               79.0|    36.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1215|   EWR| DCA|        1715|  1710.0|     -5.0|    28.0|    1738.0|   1819.0|    4.0|        1838|  1823.0|    -15.0|      0.0|             NULL|     0.0|            83.0|               73.0|    41.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1217|   EWR| DCA|        1300|  1255.0|     -5.0|    15.0|    1310.0|   1349.0|    7.0|        1408|  1356.0|    -12.0|      0.0|             NULL|     0.0|            68.0|               61.0|    39.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1218|   DCA| EWR|        1500|  1457.0|     -3.0|    14.0|    1511.0|   1552.0|    7.0|        1620|  1559.0|    -21.0|      0.0|             NULL|     0.0|            80.0|               62.0|    41.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1219|   EWR| DCA|        2135|  2131.0|     -4.0|    21.0|    2152.0|   2232.0|    3.0|        2252|  2235.0|    -17.0|      0.0|             NULL|     0.0|            77.0|               64.0|    40.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1220|   CLE| DCA|        1905|  1855.0|    -10.0|    10.0|    1905.0|   1956.0|    5.0|        2025|  2001.0|    -24.0|      0.0|             NULL|     0.0|            80.0|               66.0|    51.0|   310.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1220|   DCA| EWR|        2100|  2049.0|    -11.0|    10.0|    2059.0|   2133.0|   10.0|        2217|  2143.0|    -34.0|      0.0|             NULL|     0.0|            77.0|               54.0|    34.0|   199.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1232|   ORD| EWR|         905|   900.0|     -5.0|    16.0|     916.0|   1144.0|    6.0|        1212|  1150.0|    -22.0|      0.0|             NULL|     0.0|           127.0|              110.0|    88.0|   719.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1233|   EWR| ORD|        1000|  1035.0|     35.0|    14.0|    1049.0|   1156.0|   10.0|        1139|  1206.0|     27.0|      0.0|             NULL|     0.0|           159.0|              151.0|   127.0|   719.0|          0.0|         27.0|      0.0|           0.0|                0.0|       NULL|\n",
      "|2009-01-01|        XE|             1234|   ORD| EWR|        1230|  1234.0|      4.0|     8.0|    1242.0|   1511.0|   14.0|        1559|  1525.0|    -34.0|      0.0|             NULL|     0.0|           149.0|              111.0|    89.0|   719.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1235|   EWR| ORD|        1343|  1406.0|     23.0|    13.0|    1419.0|   1523.0|    7.0|        1523|  1530.0|      7.0|      0.0|             NULL|     0.0|           160.0|              144.0|   124.0|   719.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1236|   ORD| EWR|        1630|  1619.0|    -11.0|    19.0|    1638.0|   1906.0|   35.0|        2002|  1941.0|    -21.0|      0.0|             NULL|     0.0|           152.0|              142.0|    88.0|   719.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "|2009-01-01|        XE|             1237|   EWR| ORD|        1930|  1927.0|     -3.0|    16.0|    1943.0|   2049.0|    8.0|        2123|  2057.0|    -26.0|      0.0|             NULL|     0.0|           173.0|              150.0|   126.0|   719.0|         NULL|         NULL|     NULL|          NULL|               NULL|       NULL|\n",
      "+----------+----------+-----------------+------+----+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+-----------------+--------+----------------+-------------------+--------+--------+-------------+-------------+---------+--------------+-------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Cast CANCELLED to float\n",
    "df = df.withColumn(\"CANCELLED\", col(\"CANCELLED\").cast(\"float\"))\n",
    "\n",
    "# Filter on 0.0\n",
    "df_clean = (\n",
    "    df\n",
    "    .filter(col(\"CANCELLED\") == 0.0)\n",
    "    .dropna(subset=[\"ORIGIN\", \"DEST\"])\n",
    "    .filter(col(\"ORIGIN\") != col(\"DEST\"))\n",
    ")\n",
    "df_clean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th></tr>\n",
       "<tr><td>DCA</td></tr>\n",
       "<tr><td>GSO</td></tr>\n",
       "<tr><td>IAH</td></tr>\n",
       "<tr><td>JAX</td></tr>\n",
       "<tr><td>OKC</td></tr>\n",
       "<tr><td>CHS</td></tr>\n",
       "<tr><td>ABQ</td></tr>\n",
       "<tr><td>IND</td></tr>\n",
       "<tr><td>CLT</td></tr>\n",
       "<tr><td>CVG</td></tr>\n",
       "<tr><td>GRR</td></tr>\n",
       "<tr><td>OMA</td></tr>\n",
       "<tr><td>DAL</td></tr>\n",
       "<tr><td>LBB</td></tr>\n",
       "<tr><td>MEM</td></tr>\n",
       "<tr><td>MOB</td></tr>\n",
       "<tr><td>ICT</td></tr>\n",
       "<tr><td>GSP</td></tr>\n",
       "<tr><td>PVD</td></tr>\n",
       "<tr><td>AVL</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+---+\n",
       "| id|\n",
       "+---+\n",
       "|DCA|\n",
       "|GSO|\n",
       "|IAH|\n",
       "|JAX|\n",
       "|OKC|\n",
       "|CHS|\n",
       "|ABQ|\n",
       "|IND|\n",
       "|CLT|\n",
       "|CVG|\n",
       "|GRR|\n",
       "|OMA|\n",
       "|DAL|\n",
       "|LBB|\n",
       "|MEM|\n",
       "|MOB|\n",
       "|ICT|\n",
       "|GSP|\n",
       "|PVD|\n",
       "|AVL|\n",
       "+---+\n",
       "only showing top 20 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from graphframes import GraphFrame\n",
    "\n",
    "vertices = df_clean.select(col(\"ORIGIN\").alias(\"id\")) \\\n",
    "    .union(df_clean.select(col(\"DEST\").alias(\"id\"))) \\\n",
    "    .distinct()\n",
    "\n",
    "display(vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>src</th><th>dst</th></tr>\n",
       "<tr><td>DCA</td><td>EWR</td></tr>\n",
       "<tr><td>EWR</td><td>IAD</td></tr>\n",
       "<tr><td>EWR</td><td>DCA</td></tr>\n",
       "<tr><td>DCA</td><td>EWR</td></tr>\n",
       "<tr><td>IAD</td><td>EWR</td></tr>\n",
       "<tr><td>ATL</td><td>EWR</td></tr>\n",
       "<tr><td>CLE</td><td>ATL</td></tr>\n",
       "<tr><td>DCA</td><td>EWR</td></tr>\n",
       "<tr><td>EWR</td><td>DCA</td></tr>\n",
       "<tr><td>EWR</td><td>DCA</td></tr>\n",
       "<tr><td>DCA</td><td>EWR</td></tr>\n",
       "<tr><td>EWR</td><td>DCA</td></tr>\n",
       "<tr><td>CLE</td><td>DCA</td></tr>\n",
       "<tr><td>DCA</td><td>EWR</td></tr>\n",
       "<tr><td>ORD</td><td>EWR</td></tr>\n",
       "<tr><td>EWR</td><td>ORD</td></tr>\n",
       "<tr><td>ORD</td><td>EWR</td></tr>\n",
       "<tr><td>EWR</td><td>ORD</td></tr>\n",
       "<tr><td>ORD</td><td>EWR</td></tr>\n",
       "<tr><td>EWR</td><td>ORD</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+---+---+\n",
       "|src|dst|\n",
       "+---+---+\n",
       "|DCA|EWR|\n",
       "|EWR|IAD|\n",
       "|EWR|DCA|\n",
       "|DCA|EWR|\n",
       "|IAD|EWR|\n",
       "|ATL|EWR|\n",
       "|CLE|ATL|\n",
       "|DCA|EWR|\n",
       "|EWR|DCA|\n",
       "|EWR|DCA|\n",
       "|DCA|EWR|\n",
       "|EWR|DCA|\n",
       "|CLE|DCA|\n",
       "|DCA|EWR|\n",
       "|ORD|EWR|\n",
       "|EWR|ORD|\n",
       "|ORD|EWR|\n",
       "|EWR|ORD|\n",
       "|ORD|EWR|\n",
       "|EWR|ORD|\n",
       "+---+---+\n",
       "only showing top 20 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "edges = df_clean.select(\n",
    "    col(\"ORIGIN\").alias(\"src\"),\n",
    "    col(\"DEST\").alias(\"dst\")\n",
    ")\n",
    "\n",
    "display(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphFrame(v:[id: string], e:[src: string, dst: string])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = GraphFrame(vertices, edges)\n",
    "\n",
    "vertices.cache()\n",
    "edges.cache()\n",
    "\n",
    "display(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 1: Compute different statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>in_degree</th><th>out_degree</th><th>total_degree</th></tr>\n",
       "<tr><td>DCA</td><td>78016</td><td>77900</td><td>155916</td></tr>\n",
       "<tr><td>GSO</td><td>9869</td><td>9856</td><td>19725</td></tr>\n",
       "<tr><td>IAH</td><td>180874</td><td>180960</td><td>361834</td></tr>\n",
       "<tr><td>JAX</td><td>28593</td><td>28534</td><td>57127</td></tr>\n",
       "<tr><td>OKC</td><td>21741</td><td>21680</td><td>43421</td></tr>\n",
       "<tr><td>CHS</td><td>12659</td><td>12649</td><td>25308</td></tr>\n",
       "<tr><td>ABQ</td><td>35419</td><td>35393</td><td>70812</td></tr>\n",
       "<tr><td>IND</td><td>37726</td><td>37691</td><td>75417</td></tr>\n",
       "<tr><td>CLT</td><td>115338</td><td>115521</td><td>230859</td></tr>\n",
       "<tr><td>CVG</td><td>56462</td><td>56634</td><td>113096</td></tr>\n",
       "<tr><td>GRR</td><td>13781</td><td>13756</td><td>27537</td></tr>\n",
       "<tr><td>OMA</td><td>25294</td><td>25216</td><td>50510</td></tr>\n",
       "<tr><td>DAL</td><td>47477</td><td>47553</td><td>95030</td></tr>\n",
       "<tr><td>LBB</td><td>7891</td><td>7873</td><td>15764</td></tr>\n",
       "<tr><td>MEM</td><td>70657</td><td>70834</td><td>141491</td></tr>\n",
       "<tr><td>MOB</td><td>6564</td><td>6554</td><td>13118</td></tr>\n",
       "<tr><td>ICT</td><td>13017</td><td>12975</td><td>25992</td></tr>\n",
       "<tr><td>GSP</td><td>9939</td><td>9886</td><td>19825</td></tr>\n",
       "<tr><td>PVD</td><td>18112</td><td>18090</td><td>36202</td></tr>\n",
       "<tr><td>AVL</td><td>4493</td><td>4470</td><td>8963</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+---+---------+----------+------------+\n",
       "| id|in_degree|out_degree|total_degree|\n",
       "+---+---------+----------+------------+\n",
       "|DCA|    78016|     77900|      155916|\n",
       "|GSO|     9869|      9856|       19725|\n",
       "|IAH|   180874|    180960|      361834|\n",
       "|JAX|    28593|     28534|       57127|\n",
       "|OKC|    21741|     21680|       43421|\n",
       "|CHS|    12659|     12649|       25308|\n",
       "|ABQ|    35419|     35393|       70812|\n",
       "|IND|    37726|     37691|       75417|\n",
       "|CLT|   115338|    115521|      230859|\n",
       "|CVG|    56462|     56634|      113096|\n",
       "|GRR|    13781|     13756|       27537|\n",
       "|OMA|    25294|     25216|       50510|\n",
       "|DAL|    47477|     47553|       95030|\n",
       "|LBB|     7891|      7873|       15764|\n",
       "|MEM|    70657|     70834|      141491|\n",
       "|MOB|     6564|      6554|       13118|\n",
       "|ICT|    13017|     12975|       25992|\n",
       "|GSP|     9939|      9886|       19825|\n",
       "|PVD|    18112|     18090|       36202|\n",
       "|AVL|     4493|      4470|        8963|\n",
       "+---+---------+----------+------------+\n",
       "only showing top 20 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Compute out-degree: count flights leaving each airport.\n",
    "out_degree = edges.groupBy(\"src\").agg(count(\"*\").alias(\"out_degree\")) \\\n",
    "                  .withColumnRenamed(\"src\", \"id\")\n",
    "\n",
    "# Compute in-degree: count flights arriving at each airport.\n",
    "in_degree = edges.groupBy(\"dst\").agg(count(\"*\").alias(\"in_degree\")) \\\n",
    "                 .withColumnRenamed(\"dst\", \"id\")\n",
    "\n",
    "# Combine the results with vertices to include all airports,\n",
    "degree_df = vertices.join(in_degree, on=\"id\", how=\"left\") \\\n",
    "                    .join(out_degree, on=\"id\", how=\"left\") \\\n",
    "                    .na.fill(0) \\\n",
    "                    .withColumn(\"total_degree\", col(\"in_degree\") + col(\"out_degree\"))\n",
    "\n",
    "# Display the degree statistics\n",
    "display(degree_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triangle count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|vertex|triangleCount|\n",
      "+------+-------------+\n",
      "|   ABQ|          311|\n",
      "|   ANC|          122|\n",
      "|   BDL|          283|\n",
      "|   BOS|          779|\n",
      "|   BTV|           26|\n",
      "|   BNA|          635|\n",
      "|   CHS|           65|\n",
      "|   CLT|          695|\n",
      "|   AZO|            6|\n",
      "|   CAE|           35|\n",
      "|   CVG|          787|\n",
      "|   DCA|          411|\n",
      "|   BUR|           49|\n",
      "|   BMI|           15|\n",
      "|   CLL|            0|\n",
      "|   AVL|           20|\n",
      "|   DSM|           47|\n",
      "|   BGM|            0|\n",
      "|   DAL|           49|\n",
      "|   EGE|           25|\n",
      "+------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# triangle count, undirected\n",
    "def triangleCount(graph):\n",
    "    # Normalize edges so that each edge is stored in one direction.\n",
    "    edges = graph.edges.select(\n",
    "        F.least(\"src\", \"dst\").alias(\"src\"), \n",
    "        F.greatest(\"src\", \"dst\").alias(\"dst\")\n",
    "    ).distinct()\n",
    "\n",
    "    # 1. Find neighbors for each node.\n",
    "    neighbors = edges.groupBy(\"src\").agg(F.collect_set(\"dst\").alias(\"neighbors\"))\n",
    "\n",
    "    # 2. Join neighbors with edges to find pairs of neighbors for a node.\n",
    "    neighbor_pairs = neighbors.alias(\"n1\").join(\n",
    "        edges.alias(\"e\"), col(\"n1.src\") == col(\"e.src\")\n",
    "    ).join(\n",
    "        neighbors.alias(\"n2\"), col(\"e.dst\") == col(\"n2.src\")\n",
    "    ).select(\n",
    "        col(\"n1.src\").alias(\"node1\"),\n",
    "        col(\"n1.neighbors\").alias(\"neighbors1\"),\n",
    "        col(\"n2.src\").alias(\"node2\"),\n",
    "        col(\"n2.neighbors\").alias(\"neighbors2\")\n",
    "    )\n",
    "\n",
    "    # 3. Find intersections of neighbor sets and count.\n",
    "    triangles = neighbor_pairs.withColumn(\n",
    "        \"intersection_size\", F.size(F.array_intersect(\"neighbors1\", \"neighbors2\"))\n",
    "    ).select(\"node1\", \"node2\", \"intersection_size\")\n",
    "\n",
    "    # 4. Group by node1 to calculate triangles.\n",
    "    triangle_counts = triangles.groupBy(\"node1\").agg(\n",
    "        F.sum(\"intersection_size\").alias(\"triangleCount\")\n",
    "    ).select(col(\"node1\").alias(\"vertex\"), \"triangleCount\")\n",
    "\n",
    "    return triangle_counts\n",
    "\n",
    "# Usage\n",
    "triangle_counts = triangleCount(graph)\n",
    "triangle_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>count</th><th>id</th></tr>\n",
       "<tr><td>673</td><td>DCA</td></tr>\n",
       "<tr><td>54</td><td>GSO</td></tr>\n",
       "<tr><td>1338</td><td>IAH</td></tr>\n",
       "<tr><td>342</td><td>JAX</td></tr>\n",
       "<tr><td>165</td><td>OKC</td></tr>\n",
       "<tr><td>90</td><td>CHS</td></tr>\n",
       "<tr><td>311</td><td>ABQ</td></tr>\n",
       "<tr><td>612</td><td>IND</td></tr>\n",
       "<tr><td>1049</td><td>CLT</td></tr>\n",
       "<tr><td>1260</td><td>CVG</td></tr>\n",
       "<tr><td>96</td><td>GRR</td></tr>\n",
       "<tr><td>142</td><td>OMA</td></tr>\n",
       "<tr><td>73</td><td>DAL</td></tr>\n",
       "<tr><td>23</td><td>LBB</td></tr>\n",
       "<tr><td>1104</td><td>MEM</td></tr>\n",
       "<tr><td>6</td><td>MOB</td></tr>\n",
       "<tr><td>52</td><td>ICT</td></tr>\n",
       "<tr><td>54</td><td>GSP</td></tr>\n",
       "<tr><td>161</td><td>PVD</td></tr>\n",
       "<tr><td>27</td><td>AVL</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+-----+---+\n",
       "|count| id|\n",
       "+-----+---+\n",
       "|  673|DCA|\n",
       "|   54|GSO|\n",
       "| 1338|IAH|\n",
       "|  342|JAX|\n",
       "|  165|OKC|\n",
       "|   90|CHS|\n",
       "|  311|ABQ|\n",
       "|  612|IND|\n",
       "| 1049|CLT|\n",
       "| 1260|CVG|\n",
       "|   96|GRR|\n",
       "|  142|OMA|\n",
       "|   73|DAL|\n",
       "|   23|LBB|\n",
       "| 1104|MEM|\n",
       "|    6|MOB|\n",
       "|   52|ICT|\n",
       "|   54|GSP|\n",
       "|  161|PVD|\n",
       "|   27|AVL|\n",
       "+-----+---+\n",
       "only showing top 20 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of using from GraphFrames\n",
    "gf_trianglecount = graph.triangleCount()\n",
    "display(gf_trianglecount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 2: Total number of triangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of triangles: 15991\n"
     ]
    }
   ],
   "source": [
    "# Total triangle count using our implementation\n",
    "total_triangles = triangle_counts.agg(F.sum(\"triangleCount\").alias(\"total\")).collect()[0][\"total\"]\n",
    "print(\"Total number of triangles:\", total_triangles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of triangles: 15991.0\n"
     ]
    }
   ],
   "source": [
    "# Total triangle count counted from GraphFrames implementation\n",
    "total_sum_gf = gf_trianglecount.agg(F.sum(\"count\").alias(\"total\")).collect()[0][\"total\"]\n",
    "\n",
    "# Each triangle is counted 3 times (once per vertex), so divide by 3.\n",
    "total_triangles_gf = total_sum_gf / 3\n",
    "\n",
    "print(\"Total number of triangles:\", total_triangles_gf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 3: Closeness Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, col, when\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def compute_closeness_centrality(vertices, edges, max_depth=10):\n",
    "    closeness_scores = []\n",
    "\n",
    "    vertex_ids = [row[\"id\"] for row in vertices.select(\"id\").collect()]\n",
    "\n",
    "    for source in vertex_ids:\n",
    "        print(f\"Processing: {source}\")  # debug log\n",
    "\n",
    "        visited = vertices.withColumn(\"distance\", when(col(\"id\") == source, lit(0)))\n",
    "        frontier = visited.filter(col(\"id\") == source)\n",
    "\n",
    "        depth = 0\n",
    "        while frontier.count() > 0 and depth < max_depth:\n",
    "            print(f\"Depth: {depth}, Frontier size: {frontier.count()}\")  # debug log\n",
    "\n",
    "            next_frontier = frontier.join(edges, frontier[\"id\"] == edges[\"src\"]) \\\n",
    "                                    .select(edges[\"dst\"].alias(\"id\")) \\\n",
    "                                    .distinct()\n",
    "\n",
    "            next_frontier = next_frontier.join(visited, \"id\", \"left_anti\") \\\n",
    "                                         .withColumn(\"distance\", lit(depth + 1))\n",
    "\n",
    "            visited = visited.union(next_frontier).distinct()\n",
    "            frontier = next_frontier\n",
    "            depth += 1\n",
    "\n",
    "        reachable = visited.filter((col(\"id\") != source) & col(\"distance\").isNotNull())\n",
    "        reachable_count = reachable.count()\n",
    "\n",
    "        if reachable_count > 0:\n",
    "            total_distance = reachable.agg({\"distance\": \"sum\"}).collect()[0][0]\n",
    "            avg_distance = total_distance / reachable_count\n",
    "            closeness = 1 / avg_distance if avg_distance > 0 else 0\n",
    "        else:\n",
    "            closeness = 0\n",
    "\n",
    "        closeness_scores.append((source, closeness))\n",
    "\n",
    "    return spark.createDataFrame(closeness_scores, [\"id\", \"closeness_centrality\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import gc\n",
    "import time\n",
    "\n",
    "def compute_all_closeness_in_batches(vertices: DataFrame, edges: DataFrame, batch_size: int = 10, max_depth: int = 3) -> DataFrame:\n",
    "    from pyspark.sql.functions import col\n",
    "\n",
    "    airport_ids = [row[\"id\"] for row in vertices.collect()]\n",
    "    total = len(airport_ids)\n",
    "    all_results = []\n",
    "\n",
    "    for i in range(0, total, batch_size):\n",
    "        print(f\"\\n🧮 Batch {i // batch_size + 1}: Processing airports {i} to {min(i + batch_size, total)}...\")\n",
    "\n",
    "        batch_ids = airport_ids[i:i + batch_size]\n",
    "        batch_vertices = vertices.filter(col(\"id\").isin(batch_ids))\n",
    "\n",
    "        # Try/catch for robustness\n",
    "        try:\n",
    "            batch_result = compute_closeness_centrality(batch_vertices, edges, max_depth=max_depth)\n",
    "            all_results.append(batch_result)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in batch {i // batch_size + 1}: {e}\")\n",
    "        \n",
    "        # Give Spark a break (crucial in Docker)\n",
    "        spark.catalog.clearCache()\n",
    "        gc.collect()\n",
    "        time.sleep(2)\n",
    "\n",
    "    # Combine results safely\n",
    "    if not all_results:\n",
    "        return spark.createDataFrame([], schema=\"id STRING, closeness_centrality DOUBLE\")\n",
    "\n",
    "    combined = all_results[0]\n",
    "    for df in all_results[1:]:\n",
    "        combined = combined.union(df)\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧮 Batch 1: Processing airports 0 to 5...\n",
      "Processing: DCA\n",
      "Depth: 0, Frontier size: 1\n",
      "Depth: 1, Frontier size: 46\n",
      "Processing: GSO\n",
      "Depth: 0, Frontier size: 1\n",
      "Depth: 1, Frontier size: 10\n",
      "Processing: IAH\n",
      "Depth: 0, Frontier size: 1\n",
      "Depth: 1, Frontier size: 98\n",
      "Processing: JAX\n",
      "Depth: 0, Frontier size: 1\n",
      "Depth: 1, Frontier size: 28\n",
      "Processing: OKC\n",
      "Depth: 0, Frontier size: 1\n",
      "Depth: 1, Frontier size: 19\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>id</th><th>closeness_centrality</th></tr>\n",
       "<tr><td>IAH</td><td>0.6033755274261604</td></tr>\n",
       "<tr><td>DCA</td><td>0.5467479674796748</td></tr>\n",
       "<tr><td>JAX</td><td>0.5318181818181819</td></tr>\n",
       "<tr><td>OKC</td><td>0.518664047151277</td></tr>\n",
       "<tr><td>GSO</td><td>0.5114155251141552</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+--------------------+\n",
       "| id|closeness_centrality|\n",
       "+---+--------------------+\n",
       "|IAH|  0.6033755274261604|\n",
       "|DCA|  0.5467479674796748|\n",
       "|JAX|  0.5318181818181819|\n",
       "|OKC|   0.518664047151277|\n",
       "|GSO|  0.5114155251141552|\n",
       "+---+--------------------+"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "closeness_df_batched = compute_all_closeness_in_batches(vertices.limit(5), edges, batch_size=5, max_depth=2)\n",
    "display(closeness_df_batched.orderBy(col(\"closeness_centrality\").desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total edges: 6342300\n",
      "+---+---+\n",
      "|src|dst|\n",
      "+---+---+\n",
      "|DCA|EWR|\n",
      "|EWR|IAD|\n",
      "|EWR|DCA|\n",
      "|DCA|EWR|\n",
      "|IAD|EWR|\n",
      "+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Total edges:\", edges.count())\n",
    "edges.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges with unmatched nodes: 0\n",
      "+---+---+\n",
      "|src|dst|\n",
      "+---+---+\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length, trim\n",
    "\n",
    "# Check for edge values not in vertices\n",
    "edges_not_in_vertices = edges.filter(~col(\"src\").isin([row[\"id\"] for row in vertices.collect()]) |\n",
    "                                     ~col(\"dst\").isin([row[\"id\"] for row in vertices.collect()]))\n",
    "\n",
    "print(\"Edges with unmatched nodes:\", edges_not_in_vertices.count())\n",
    "edges_not_in_vertices.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From ATL, reachable nodes within depth 3: 296\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "airport = \"ATL\"  # a well-connected airport\n",
    "\n",
    "frontier = deque([airport])\n",
    "visited = set()\n",
    "depth = 0\n",
    "max_depth = 3\n",
    "\n",
    "while frontier and depth < max_depth:\n",
    "    next_level = deque()\n",
    "    for node in frontier:\n",
    "        neighbors = edges.filter(edges[\"src\"] == node).select(\"dst\").rdd.flatMap(lambda x: x).collect()\n",
    "        for n in neighbors:\n",
    "            if n not in visited:\n",
    "                next_level.append(n)\n",
    "                visited.add(n)\n",
    "    frontier = next_level\n",
    "    depth += 1\n",
    "\n",
    "print(f\"From {airport}, reachable nodes within depth {max_depth}: {len(visited)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 4: PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|              rank|\n",
      "+---+------------------+\n",
      "|ATL| 18.90331756378957|\n",
      "|ORD|12.820461217499425|\n",
      "|DFW| 11.68222860838244|\n",
      "|DEN|10.015676869655355|\n",
      "|LAX| 7.746260972016691|\n",
      "|IAH| 7.211790885985897|\n",
      "|PHX| 7.089867862173453|\n",
      "|SLC| 7.069727723781594|\n",
      "|DTW| 7.039411046783919|\n",
      "|SFO| 5.904332525116082|\n",
      "|MSP|5.7766491361090395|\n",
      "|LAS| 5.711270775494287|\n",
      "|SEA| 5.051625259564932|\n",
      "|JFK| 4.536582814940282|\n",
      "|MCO| 4.488214748918565|\n",
      "|EWR| 4.304072390812124|\n",
      "|CLT|  4.24882537718414|\n",
      "|BOS|3.8160145173142066|\n",
      "|BWI| 3.634644704603932|\n",
      "|LGA|3.5233818261136656|\n",
      "+---+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark.sparkContext.setCheckpointDir(\"/tmp/spark-checkpoints\")\n",
    "\n",
    "# assign the default rank 1 to each vertex\n",
    "ranks = vertices.withColumn(\"rank\", F.lit(1.0))\n",
    "ranks = ranks.checkpoint(eager=True)\n",
    "N = vertices.count()\n",
    "\n",
    "# run algorithm for 10 iterations\n",
    "iterations = 10\n",
    "# damping factor\n",
    "damping = 0.85\n",
    "\n",
    "for i in range (iterations):\n",
    "    # calculate the contributions each vertex receives\n",
    "    contributions = (edges.join(ranks, edges.src == ranks.id)\n",
    "                     .join(out_degree, edges.src == out_degree.id) # get all the vertices and number of outgoing edges\n",
    "                     .select(\n",
    "                         edges.dst.alias(\"id\"),\n",
    "                         (ranks[\"rank\"] / out_degree[\"out_degree\"]).alias(\"contrib\") # contribution the destination vertex receives\n",
    "                     ))\n",
    "\n",
    "    # sum up contribution each node received \n",
    "    grouped_contributions = (contributions.groupBy(\"id\")\n",
    "              .agg(F.sum(\"contrib\").alias(\"sum_contrib\"))\n",
    "              .select(\n",
    "                  col(\"id\"),\n",
    "                  (((1 - damping) / N) + damping * col(\"sum_contrib\")).alias(\"rank\") # apply pagerank formula\n",
    "              ))\n",
    "\n",
    "    # assigm default rank to the nodes that did not receive any contributions\n",
    "    dangling = vertices.join(grouped_contributions.select(\"id\"), \"id\", \"left_anti\") \\\n",
    "                       .withColumn(\"rank\", F.lit((1 - damping) / N))\n",
    "\n",
    "    # update the ranks and make checkpoint to prevent OOM errors\n",
    "    new_ranks = summed.unionByName(dangling)\n",
    "    ranks = new_ranks.checkpoint(eager=True)\n",
    "\n",
    "\n",
    "# Show final ranks\n",
    "ranks.orderBy(\"rank\", ascending=False).show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
