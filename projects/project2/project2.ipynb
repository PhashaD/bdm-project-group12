{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de047a38-145e-4c9c-b358-97472554bcc4",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b21e607-fcbb-480b-9abe-3f9536cb0b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: delta-spark in /opt/conda/lib/python3.11/site-packages (3.3.0)\n",
      "Requirement already satisfied: pyspark<3.6.0,>=3.5.3 in /usr/local/spark/python (from delta-spark) (3.5.3)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from delta-spark) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata>=1.0.0->delta-spark) (3.20.2)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark<3.6.0,>=3.5.3->delta-spark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install delta-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7cadb3c-ae8f-4cf8-9b74-477eea83109e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "builder = SparkSession.builder.appName(\"project2\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6a9f55-9d79-4a43-a3eb-2a746e9bc4f6",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e203a57-848a-4c19-9e64-74f1fc02d118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- trip_time_in_secs: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- surcharge: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the expected column names based on the CSV's field count\n",
    "columns = [\n",
    "    \"medallion\", \"hack_license\", \"pickup_datetime\", \"dropoff_datetime\",\n",
    "    \"trip_time_in_secs\", \"trip_distance\", \"pickup_longitude\", \"pickup_latitude\",\n",
    "    \"dropoff_longitude\", \"dropoff_latitude\", \"payment_type\", \"fare_amount\",\n",
    "    \"surcharge\", \"mta_tax\", \"tip_amount\", \"tolls_amount\", \"total_amount\"\n",
    "]\n",
    "\n",
    "df = spark.read.option(\"header\", \"false\").csv(\"input/sorted_data.csv\")\n",
    "df = df.toDF(*columns)\n",
    "df.printSchema()\n",
    "\n",
    "fraction = 1.0 / 12  # Approximately 0.03\n",
    "df_sample = df.sample(withReplacement=False, fraction=fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee564ad2-117b-4678-9fc6-79c2b16b8c79",
   "metadata": {},
   "source": [
    "## Data cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a46348fb-cb0f-46c2-8367-d40e5b355a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|           medallion|        hack_license|    pickup_datetime|   dropoff_datetime|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|0CEBE42EAF42C3380...|CC7A4176549BA819E...|2013-01-01 00:00:00|2013-01-01 00:03:00|              180|         1.56|      -74.009750|      40.706432|       -73.971985|       40.794716|         CSH|        6.5|      0.5|    0.5|       0.0|         0.0|        7.50|\n",
      "|5CC9B3C9725FCD7FA...|9F4A98FA581907FC2...|2013-01-01 00:00:00|2013-01-01 00:03:00|              180|         0.64|      -74.001404|      40.722610|       -73.998177|       40.729485|         CSH|        4.5|      0.5|    0.5|       0.0|         0.0|        5.50|\n",
      "|E07A1976036B82DA9...|F33756EA65252C595...|2013-01-01 00:01:00|2013-01-01 00:03:00|              120|         0.01|      -73.982216|      40.768768|       -73.982132|       40.768639|         CSH|        3.0|      0.5|    0.5|       0.0|         0.0|        4.00|\n",
      "|7EEAE7CDAFC4D42FE...|1BB1BDE9F4C40D094...|2013-01-01 00:01:35|2013-01-01 00:03:44|              129|          0.8|      -73.923775|      40.770325|       -73.931877|       40.759815|         CSH|        4.5|      0.5|    0.5|       0.0|         0.0|        5.50|\n",
      "|87EC61B520189EFEA...|2B9F6F2C53DF205A4...|2013-01-01 00:02:00|2013-01-01 00:04:00|              120|         0.72|      -73.977089|      40.789925|       -73.984505|       40.782444|         CSH|        4.5|      0.5|    0.5|       0.0|         0.0|        5.50|\n",
      "|EEF7C5160FE5886A2...|9441C416CC0FB7BA6...|2013-01-01 00:03:02|2013-01-01 00:05:11|              128|          0.6|      -73.988525|      40.758873|       -73.982552|       40.767254|         CSH|        4.0|      0.5|    0.5|       0.0|         0.0|        5.00|\n",
      "|1691DC222DB9054B1...|2F0A73204DFEEA265...|2013-01-01 00:01:31|2013-01-01 00:05:54|              263|          0.5|      -73.987053|      40.739876|       -73.992851|       40.742748|         CSH|        4.5|      0.5|    0.5|       0.0|         0.0|        5.50|\n",
      "|5F9C08A05F6449EE3...|7CA54E7DDBB0D2882...|2013-01-01 00:01:00|2013-01-01 00:06:00|              300|         1.32|      -73.959694|      40.776882|       -73.961655|       40.763577|         CSH|        6.5|      0.5|    0.5|       0.0|         0.0|        7.50|\n",
      "|5142072568D80D822...|CB79FA15C572A8F0A...|2013-01-01 00:04:35|2013-01-01 00:06:06|               91|          0.4|      -73.968758|      40.754688|       -73.974396|       40.757286|         CSH|        3.5|      0.5|    0.5|       0.0|         0.0|        4.50|\n",
      "|1C341430AF45F688E...|9F7118CDF1876A0E4...|2013-01-01 00:05:01|2013-01-01 00:06:36|               94|          0.2|      -74.005356|      40.720814|       -74.004868|       40.723621|         CSH|        3.0|      0.5|    0.5|       0.0|         0.0|        4.00|\n",
      "|1F9E2C27335E24C6A...|F0CB659C0B6D2FABC...|2013-01-01 00:03:00|2013-01-01 00:07:00|              240|         0.26|      -73.996834|      40.720367|       -73.994102|       40.721085|         CSH|        4.0|      0.5|    0.5|       0.0|         0.0|        5.00|\n",
      "|9836A6780886D5D11...|59E4699CBA26E3BCA...|2013-01-01 00:05:00|2013-01-01 00:07:00|              120|         0.61|      -73.994644|      40.756016|       -73.998413|       40.750599|         CRD|        4.0|      0.5|    0.5|      1.35|         0.0|        6.35|\n",
      "|312E0CB058D7FC1A6...|59F06ED612FD58E8C...|2013-01-01 00:00:14|2013-01-01 00:07:10|              416|          0.7|      -73.993736|      40.752232|       -73.996468|       40.742966|         CRD|        6.5|      0.5|    0.5|       2.0|         0.0|        9.50|\n",
      "|67B0EC98DD6BCB83F...|D8B9D865960E24575...|2013-01-01 00:03:53|2013-01-01 00:07:28|              215|          1.2|      -74.001938|      40.726345|       -73.993599|       40.741795|         CRD|        5.5|      0.5|    0.5|       1.3|         0.0|        7.80|\n",
      "|586E42CBE3010646C...|83F57D0B7B416F514...|2013-01-01 00:03:11|2013-01-01 00:07:41|              270|          1.5|      -73.959038|      40.780590|       -73.951042|       40.798401|         CSH|        6.5|      0.5|    0.5|       0.0|         0.0|        7.50|\n",
      "|30805D66D3BC8565D...|17278280E0FBA6329...|2013-01-01 00:01:22|2013-01-01 00:07:53|              391|          1.3|      -73.997574|      40.760525|       -74.006332|       40.744595|         CSH|        7.5|      0.5|    0.5|       0.0|         0.0|        8.50|\n",
      "|FD487D4BE7A6709FF...|F879798C97EE20CA7...|2013-01-01 00:05:30|2013-01-01 00:07:56|              145|          0.7|      -73.976677|      40.751637|       -73.977242|       40.742889|         CSH|        4.5|      0.5|    0.5|       0.0|         0.0|        5.50|\n",
      "|0F6CF8A85A039CDF3...|B6ECF5802089A7125...|2013-01-01 00:01:00|2013-01-01 00:08:00|              420|         1.95|      -73.983185|      40.730606|       -73.977295|       40.751850|         CSH|        8.5|      0.5|    0.5|       0.0|         0.0|        9.50|\n",
      "|1E382090C15DF8B41...|6DC841A5F028073A7...|2013-01-01 00:07:00|2013-01-01 00:08:00|               60|         0.01|      -74.000648|      40.718609|       -74.000755|       40.718674|         CSH|        3.0|      0.5|    0.5|       0.0|         0.0|        4.00|\n",
      "|F1D1F07C119A09262...|333577CC68DC3366D...|2013-01-01 00:03:00|2013-01-01 00:08:00|              300|         1.44|      -73.965485|      40.805790|       -73.955780|       40.818932|         CSH|        7.0|      0.5|    0.5|       0.0|         0.0|        8.00|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "# Convert pickup and dropoff datetimes to timestamps\n",
    "df_sample = df_sample.withColumn(\"pickup_datetime\", to_timestamp(col(\"pickup_datetime\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "                     .withColumn(\"dropoff_datetime\", to_timestamp(col(\"dropoff_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "clean_df = df_sample.filter(\n",
    "    (col(\"pickup_datetime\").isNotNull()) &\n",
    "    (col(\"dropoff_datetime\").isNotNull()) &\n",
    "    (col(\"medallion\").isNotNull()) & (col(\"medallion\") != \"\") &\n",
    "    (col(\"hack_license\").isNotNull()) & (col(\"hack_license\") != \"\") &\n",
    "    (col(\"trip_distance\").cast(\"float\") > 0) &\n",
    "    (col(\"fare_amount\").cast(\"float\") > 0)\n",
    ")\n",
    "\n",
    "clean_df = clean_df.withColumn(\"trip_time_in_secs\", col(\"trip_time_in_secs\").cast(\"int\")) \\\n",
    "                   .withColumn(\"trip_distance\", col(\"trip_distance\").cast(\"float\")) \\\n",
    "                   .withColumn(\"fare_amount\", col(\"fare_amount\").cast(\"float\")) \\\n",
    "                   .withColumn(\"surcharge\", col(\"surcharge\").cast(\"float\")) \\\n",
    "                   .withColumn(\"mta_tax\", col(\"mta_tax\").cast(\"float\")) \\\n",
    "                   .withColumn(\"tip_amount\", col(\"tip_amount\").cast(\"float\")) \\\n",
    "                   .withColumn(\"tolls_amount\", col(\"tolls_amount\").cast(\"float\"))\n",
    "clean_df.count()\n",
    "clean_df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54a3e5f-f084-43bb-8956-6f7f33b97772",
   "metadata": {},
   "source": [
    "## Query 1 - Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11319598-cc2a-48ea-99e1-865c041e8aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max dropoff datetime: 2014-01-01 01:10:00\n",
      "+----------+--------+---------------+\n",
      "|start_cell|end_cell|Number_of_Rides|\n",
      "+----------+--------+---------------+\n",
      "|154.159   |156.159 |1              |\n",
      "|154.165   |155.157 |1              |\n",
      "|154.159   |165.171 |1              |\n",
      "|156.166   |156.166 |1              |\n",
      "+----------+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, floor, concat, lit, current_timestamp, expr, max as spark_max\n",
    "from datetime import timedelta\n",
    "\n",
    "max_dropoff = clean_df.select(spark_max(\"dropoff_datetime\")).first()[0]\n",
    "print(\"Max dropoff datetime:\", max_dropoff)\n",
    "\n",
    "ref_time = max_dropoff - timedelta(minutes=30)\n",
    "\n",
    "# Constants for the 500m x 500m grid\n",
    "grid_origin_lat = 41.474937\n",
    "grid_origin_lon = -74.913585\n",
    "delta_lat = 0.0045   # Approximate degrees for 500m in latitude\n",
    "delta_lon = 0.0060   # Approximate degrees for 500m in longitude\n",
    "\n",
    "df_routes = clean_df.withColumn(\n",
    "    \"start_cell\",\n",
    "    concat(\n",
    "        (floor((col(\"pickup_longitude\") - lit(grid_origin_lon)) / lit(delta_lon)) + 1).cast(\"int\"),\n",
    "        lit(\".\"),\n",
    "        (floor((lit(grid_origin_lat) - col(\"pickup_latitude\")) / lit(delta_lat)) + 1).cast(\"int\")\n",
    "    )\n",
    ").withColumn(\n",
    "    \"end_cell\",\n",
    "    concat(\n",
    "        (floor((col(\"dropoff_longitude\") - lit(grid_origin_lon)) / lit(delta_lon)) + 1).cast(\"int\"),\n",
    "        lit(\".\"),\n",
    "        (floor((lit(grid_origin_lat) - col(\"dropoff_latitude\")) / lit(delta_lat)) + 1).cast(\"int\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_last30 = df_routes.filter(col(\"dropoff_datetime\") >= lit(ref_time))\n",
    "\n",
    "\n",
    "# Group by start and end cell and count the rides\n",
    "df_frequent_routes = df_last30.groupBy(\"start_cell\", \"end_cell\").count() \\\n",
    "    .withColumnRenamed(\"count\", \"Number_of_Rides\")\n",
    "\n",
    "top10_routes = df_frequent_routes.orderBy(col(\"Number_of_Rides\").desc()).limit(10)\n",
    "top10_routes.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aa4abb-f52a-40b4-b71e-c37c552a757f",
   "metadata": {},
   "source": [
    "## Query 1 - Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16784a00-324c-4192-bc63-3ee8bf0a1076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the cleansed data to a Delta table in a writable directory.\n",
    "clean_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta/taxi_data\")\n",
    "streaming_df = spark.readStream.format(\"delta\").load(\"/tmp/delta/taxi_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fe1282a-f816-4eba-b092-a17b31f5bd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- trip_time_in_secs: integer (nullable = true)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: float (nullable = true)\n",
      " |-- surcharge: float (nullable = true)\n",
      " |-- mta_tax: float (nullable = true)\n",
      " |-- tip_amount: float (nullable = true)\n",
      " |-- tolls_amount: float (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      "\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|medallion                       |hack_license                    |pickup_datetime    |dropoff_datetime   |trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|C922EBA2C57BFB7683EF4457D08842D7|8347AF7A4DFA543EE7CC8558639658AA|2013-01-02 18:23:00|2013-01-02 18:59:00|2160             |10.58        |-73.870827      |40.773743      |-73.982346       |40.771576       |CSH         |35.5       |1.0      |0.5    |0.0       |4.8         |41.80       |\n",
      "|D6C88A1D929659923F068E17046036C9|4A41A45650E6EDCC62827AAD7B3C5A33|2013-01-02 18:53:00|2013-01-02 18:59:00|360              |0.81         |-73.991997      |40.731956      |-74.000298       |40.727211       |CSH         |6.0        |1.0      |0.5    |0.0       |0.0         |7.50        |\n",
      "|9CCF07B476B482C2050A8C63360586F7|8ECD713708411D7A6476EB7827E54ADD|2013-01-02 18:51:25|2013-01-02 18:59:11|466              |1.9          |-74.002762      |40.727119      |-74.003677       |40.747734       |CRD         |8.5        |1.0      |0.5    |2.0       |0.0         |12.00       |\n",
      "|B5871D0837BD95C235878A8006636456|D731BD334154A9F3DF1365F11BEAD8DF|2013-01-02 18:52:59|2013-01-02 18:59:14|374              |0.6          |-73.981201      |40.765656      |-73.982079       |40.772354       |CSH         |5.5        |1.0      |0.5    |0.0       |0.0         |7.00        |\n",
      "|AA04CED9E568816005A27A136172D491|1D4D47582D500E4DC0CAD1D2930887B3|2013-01-02 18:56:09|2013-01-02 18:59:15|185              |0.7          |-74.002625      |40.739571      |-73.995148       |40.749908       |CRD         |4.5        |1.0      |0.5    |1.2       |0.0         |7.20        |\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Row count: 14336868\n"
     ]
    }
   ],
   "source": [
    "# 1) Read the Delta table in batch mode\n",
    "check_df = spark.read.format(\"delta\").load(\"/tmp/delta/taxi_data\")\n",
    "\n",
    "# 2) See the schema\n",
    "check_df.printSchema()\n",
    "\n",
    "# 3) Inspect some rows\n",
    "check_df.show(5, truncate=False)\n",
    "\n",
    "# 4) Count the rows\n",
    "print(\"Row count:\", check_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05031d45-276b-4917-a94d-1205c449b158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window filter: dropoff_datetime >= 2014-01-01 00:40:00\n",
      "df_last30 count = 4\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+--------------------+----------------+-----------------+----------+-----------------+------------------+--------+\n",
      "|           medallion|        hack_license|    pickup_datetime|   dropoff_datetime|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total_amount|         ingest_time|pickup_cell_east|pickup_cell_south|start_cell|dropoff_cell_east|dropoff_cell_south|end_cell|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+--------------------+----------------+-----------------+----------+-----------------+------------------+--------+\n",
      "|489ED3944E97C1A5C...|797E4C99B45BCD6F7...|2013-12-31 23:56:00|2014-01-01 00:41:00|             2700|        16.32|      -73.980461|      40.730232|       -73.980141|       40.729443|         CSH|       51.5|      0.5|    0.5|       0.0|         0.0|       52.50|2025-03-30 19:01:...|             156|              166|   156.166|              156|               166| 156.166|\n",
      "|E15F7CCB808DD15E0...|006114F940CB87B3A...|2013-12-31 23:49:00|2014-01-01 00:42:00|             3180|        14.05|      -73.989738|      40.734219|       -73.987831|       40.768898|         CRD|       48.5|      0.5|    0.5|       0.0|         0.0|       49.50|2025-03-30 19:01:...|             154|              165|   154.165|              155|               157| 155.157|\n",
      "|34A6D6B37688CD29D...|17F69C23B41CEC6CD...|2013-12-31 23:52:00|2014-01-01 00:49:00|             3420|         2.31|      -73.990784|      40.760857|       -73.979645|       40.761463|         CSH|       32.0|      0.5|    0.5|       0.0|         0.0|       33.00|2025-03-30 19:01:...|             154|              159|   154.159|              156|               159| 156.159|\n",
      "|1944C571670999651...|21D54F0DB50FB1C51...|2013-12-31 23:47:00|2014-01-01 01:10:00|             4980|        17.01|      -73.992378|      40.761696|       -73.925484|       40.708118|         CRD|       69.0|      0.5|    0.5|       5.0|        5.33|       80.33|2025-03-30 19:01:...|             154|              159|   154.159|              165|               171| 165.171|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+--------------------+----------------+-----------------+----------+-----------------+------------------+--------+\n",
      "\n",
      "Update for batch 0 : {'pickup_datetime': datetime.datetime(2013, 12, 31, 23, 47), 'dropoff_datetime': datetime.datetime(2014, 1, 1, 1, 10), 'delay': 46.156276, 'start_cell_id_1': '154.159', 'end_cell_id_1': '156.159', 'start_cell_id_2': '154.159', 'end_cell_id_2': '165.171', 'start_cell_id_3': '154.165', 'end_cell_id_3': '155.157', 'start_cell_id_4': '156.166', 'end_cell_id_4': '156.166', 'start_cell_id_5': None, 'end_cell_id_5': None, 'start_cell_id_6': None, 'end_cell_id_6': None, 'start_cell_id_7': None, 'end_cell_id_7': None, 'start_cell_id_8': None, 'end_cell_id_8': None, 'start_cell_id_9': None, 'end_cell_id_9': None, 'start_cell_id_10': None, 'end_cell_id_10': None}\n",
      "+-------------------+-------------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+----------------+--------------+---------+\n",
      "|pickup_datetime    |dropoff_datetime   |start_cell_id_1|end_cell_id_1|start_cell_id_2|end_cell_id_2|start_cell_id_3|end_cell_id_3|start_cell_id_4|end_cell_id_4|start_cell_id_5|end_cell_id_5|start_cell_id_6|end_cell_id_6|start_cell_id_7|end_cell_id_7|start_cell_id_8|end_cell_id_8|start_cell_id_9|end_cell_id_9|start_cell_id_10|end_cell_id_10|delay    |\n",
      "+-------------------+-------------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+----------------+--------------+---------+\n",
      "|2013-12-31 23:47:00|2014-01-01 01:10:00|154.159        |156.159      |154.159        |165.171      |154.165        |155.157      |156.166        |156.166      |NULL           |NULL         |NULL           |NULL         |NULL           |NULL         |NULL           |NULL         |NULL           |NULL         |NULL            |NULL          |46.156276|\n",
      "+-------------------+-------------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+----------------+--------------+---------+\n",
      "\n",
      "10 most profitable areas:\n",
      "Area 1:\n",
      "  Cell ID: 330.341\n",
      "  Empty Taxis: 1\n",
      "  Median Profit: None\n",
      "  Profitability: None\n",
      "Area 2:\n",
      "  Cell ID: 308.317\n",
      "  Empty Taxis: None\n",
      "  Median Profit: 74.0\n",
      "  Profitability: None\n",
      "Area 3:\n",
      "  Cell ID: 309.314\n",
      "  Empty Taxis: 1\n",
      "  Median Profit: None\n",
      "  Profitability: None\n",
      "Area 4:\n",
      "  Cell ID: 312.318\n",
      "  Empty Taxis: 1\n",
      "  Median Profit: None\n",
      "  Profitability: None\n",
      "Area 5:\n",
      "  Cell ID: 312.332\n",
      "  Empty Taxis: 1\n",
      "  Median Profit: None\n",
      "  Profitability: None\n",
      "Update for batch 0: {'pickup_datetime': datetime.datetime(2013, 12, 31, 23, 47), 'dropoff_datetime': datetime.datetime(2014, 1, 1, 1, 10), 'delay': 32.739564, 'profitable_cell_id_1': '330.341', 'empty_taxies_in_cell_id_1': 1, 'median_profit_in_cell_id_1': None, 'profitability_of_cell_1': None, 'profitable_cell_id_2': '308.317', 'empty_taxies_in_cell_id_2': None, 'median_profit_in_cell_id_2': 74.0, 'profitability_of_cell_2': None, 'profitable_cell_id_3': '309.314', 'empty_taxies_in_cell_id_3': 1, 'median_profit_in_cell_id_3': None, 'profitability_of_cell_3': None, 'profitable_cell_id_4': '312.318', 'empty_taxies_in_cell_id_4': 1, 'median_profit_in_cell_id_4': None, 'profitability_of_cell_4': None, 'profitable_cell_id_5': '312.332', 'empty_taxies_in_cell_id_5': 1, 'median_profit_in_cell_id_5': None, 'profitability_of_cell_5': None, 'profitable_cell_id_6': None, 'empty_taxies_in_cell_id_6': None, 'median_profit_in_cell_id_6': None, 'profitability_of_cell_6': None, 'profitable_cell_id_7': None, 'empty_taxies_in_cell_id_7': None, 'median_profit_in_cell_id_7': None, 'profitability_of_cell_7': None, 'profitable_cell_id_8': None, 'empty_taxies_in_cell_id_8': None, 'median_profit_in_cell_id_8': None, 'profitability_of_cell_8': None, 'profitable_cell_id_9': None, 'empty_taxies_in_cell_id_9': None, 'median_profit_in_cell_id_9': None, 'profitability_of_cell_9': None, 'profitable_cell_id_10': None, 'empty_taxies_in_cell_id_10': None, 'median_profit_in_cell_id_10': None, 'profitability_of_cell_10': None}\n",
      "+-------------------+-------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+---------------------+--------------------------+---------------------------+------------------------+---------+\n",
      "|pickup_datetime    |dropoff_datetime   |profitable_cell_id_1|empty_taxies_in_cell_id_1|median_profit_in_cell_id_1|profitability_of_cell_1|profitable_cell_id_2|empty_taxies_in_cell_id_2|median_profit_in_cell_id_2|profitability_of_cell_2|profitable_cell_id_3|empty_taxies_in_cell_id_3|median_profit_in_cell_id_3|profitability_of_cell_3|profitable_cell_id_4|empty_taxies_in_cell_id_4|median_profit_in_cell_id_4|profitability_of_cell_4|profitable_cell_id_5|empty_taxies_in_cell_id_5|median_profit_in_cell_id_5|profitability_of_cell_5|profitable_cell_id_6|empty_taxies_in_cell_id_6|median_profit_in_cell_id_6|profitability_of_cell_6|profitable_cell_id_7|empty_taxies_in_cell_id_7|median_profit_in_cell_id_7|profitability_of_cell_7|profitable_cell_id_8|empty_taxies_in_cell_id_8|median_profit_in_cell_id_8|profitability_of_cell_8|profitable_cell_id_9|empty_taxies_in_cell_id_9|median_profit_in_cell_id_9|profitability_of_cell_9|profitable_cell_id_10|empty_taxies_in_cell_id_10|median_profit_in_cell_id_10|profitability_of_cell_10|delay    |\n",
      "+-------------------+-------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+---------------------+--------------------------+---------------------------+------------------------+---------+\n",
      "|2013-12-31 23:47:00|2014-01-01 01:10:00|330.341             |1                        |NULL                      |NULL                   |308.317             |NULL                     |74.0                      |NULL                   |309.314             |1                        |NULL                      |NULL                   |312.318             |1                        |NULL                      |NULL                   |312.332             |1                        |NULL                      |NULL                   |NULL                |NULL                     |NULL                      |NULL                   |NULL                |NULL                     |NULL                      |NULL                   |NULL                |NULL                     |NULL                      |NULL                   |NULL                |NULL                     |NULL                      |NULL                   |NULL                 |NULL                      |NULL                       |NULL                    |32.739564|\n",
      "+-------------------+-------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+---------------------+--------------------------+---------------------------+------------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, floor, concat, lit, current_timestamp, to_timestamp, expr\n",
    ")\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import timedelta, datetime\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, DoubleType, StringType\n",
    "\n",
    "# Constants for the 500m x 500m grid\n",
    "grid_origin_lat = 41.474937\n",
    "grid_origin_lon = -74.913585\n",
    "delta_lat = 0.0045   # Approximate degrees for 500m in latitude\n",
    "delta_lon = 0.0060   # Approximate degrees for 500m in longitude\n",
    "\n",
    "# This is your foreachBatch function to process each micro-batch\n",
    "def process_batch(batch_df, batch_id):\n",
    "    # Skip empty batches\n",
    "    if batch_df.rdd.isEmpty():\n",
    "        return\n",
    "\n",
    "    # Get the latest dropoff time in the batch\n",
    "    max_dropoff = batch_df.select(F.max(\"dropoff_datetime\")).first()[0]\n",
    "    if max_dropoff is None:\n",
    "        return\n",
    "    ref_time = max_dropoff - timedelta(minutes=30)\n",
    "\n",
    "    # Compute the cell IDs\n",
    "    batch_df = batch_df.withColumn(\n",
    "        \"pickup_cell_east\", floor((col(\"pickup_longitude\") - lit(grid_origin_lon)) / lit(delta_lon)) + 1\n",
    "    ).withColumn(\n",
    "        \"pickup_cell_south\", floor((lit(grid_origin_lat) - col(\"pickup_latitude\")) / lit(delta_lat)) + 1\n",
    "    ).withColumn(\n",
    "        \"start_cell\", concat(col(\"pickup_cell_east\").cast(\"int\"), lit(\".\"), col(\"pickup_cell_south\").cast(\"int\"))\n",
    "    )\n",
    "    batch_df = batch_df.withColumn(\n",
    "        \"dropoff_cell_east\", floor((col(\"dropoff_longitude\") - lit(grid_origin_lon)) / lit(delta_lon)) + 1\n",
    "    ).withColumn(\n",
    "        \"dropoff_cell_south\", floor((lit(grid_origin_lat) - col(\"dropoff_latitude\")) / lit(delta_lat)) + 1\n",
    "    ).withColumn(\n",
    "        \"end_cell\", concat(col(\"dropoff_cell_east\").cast(\"int\"), lit(\".\"), col(\"dropoff_cell_south\").cast(\"int\"))\n",
    "    )\n",
    "\n",
    "    # Filter out trips that are out-of-bounds (only consider cells 1 to 300)\n",
    "    batch_df = batch_df.filter(\n",
    "        (col(\"pickup_cell_east\").between(1, 300)) &\n",
    "        (col(\"pickup_cell_south\").between(1, 300)) &\n",
    "        (col(\"dropoff_cell_east\").between(1, 300)) &\n",
    "        (col(\"dropoff_cell_south\").between(1, 300))\n",
    "    )\n",
    "\n",
    "    # Filter for trips with dropoff_datetime >= ref_time (last 30 minutes)\n",
    "    df_last30 = batch_df.filter(col(\"dropoff_datetime\") >= F.lit(ref_time))\n",
    "    print(f\"Window filter: dropoff_datetime >= {ref_time}\")\n",
    "    print(\"df_last30 count =\", df_last30.count())\n",
    "    df_last30.show(5)\n",
    "\n",
    "    # Aggregate routes and get top 10 most frequent\n",
    "    df_frequent_routes = df_last30.groupBy(\"start_cell\", \"end_cell\").count().withColumnRenamed(\"count\", \"Number_of_Rides\")\n",
    "    top10_routes = df_frequent_routes.orderBy(col(\"Number_of_Rides\").desc()).limit(10)\n",
    "    top10_list = top10_routes.collect()\n",
    "\n",
    "    # Determine a triggering event and compute delay\n",
    "    # Choose the event with the maximum dropoff_datetime as the trigger\n",
    "    trigger_row = df_last30.orderBy(col(\"dropoff_datetime\").desc()).limit(1).collect()[0]\n",
    "    trigger_pickup = trigger_row[\"pickup_datetime\"]\n",
    "    trigger_dropoff = trigger_row[\"dropoff_datetime\"]\n",
    "    \n",
    "    ingest_time = trigger_row[\"ingest_time\"]\n",
    "    processing_time = datetime.now()\n",
    "    delay = (processing_time - ingest_time).total_seconds()\n",
    "\n",
    "    # Build the output row\n",
    "    output_row = {\n",
    "        \"pickup_datetime\": trigger_pickup,\n",
    "        \"dropoff_datetime\": trigger_dropoff,\n",
    "        \"delay\": delay\n",
    "    }\n",
    "    for i in range(10):\n",
    "        if i < len(top10_list):\n",
    "            route = top10_list[i]\n",
    "            output_row[f\"start_cell_id_{i+1}\"] = route[\"start_cell\"]\n",
    "            output_row[f\"end_cell_id_{i+1}\"] = route[\"end_cell\"]\n",
    "        else:\n",
    "            output_row[f\"start_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"end_cell_id_{i+1}\"] = None\n",
    "\n",
    "    # Print the output update\n",
    "    print(f\"Update for batch {batch_id} :\", output_row)\n",
    "    \n",
    "    # Define the output schema\n",
    "    output_schema = StructType([\n",
    "        StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "        StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "        StructField(\"start_cell_id_1\", StringType(), True),\n",
    "        StructField(\"end_cell_id_1\", StringType(), True),\n",
    "        StructField(\"start_cell_id_2\", StringType(), True),\n",
    "        StructField(\"end_cell_id_2\", StringType(), True),\n",
    "        StructField(\"start_cell_id_3\", StringType(), True),\n",
    "        StructField(\"end_cell_id_3\", StringType(), True),\n",
    "        StructField(\"start_cell_id_4\", StringType(), True),\n",
    "        StructField(\"end_cell_id_4\", StringType(), True),\n",
    "        StructField(\"start_cell_id_5\", StringType(), True),\n",
    "        StructField(\"end_cell_id_5\", StringType(), True),\n",
    "        StructField(\"start_cell_id_6\", StringType(), True),\n",
    "        StructField(\"end_cell_id_6\", StringType(), True),\n",
    "        StructField(\"start_cell_id_7\", StringType(), True),\n",
    "        StructField(\"end_cell_id_7\", StringType(), True),\n",
    "        StructField(\"start_cell_id_8\", StringType(), True),\n",
    "        StructField(\"end_cell_id_8\", StringType(), True),\n",
    "        StructField(\"start_cell_id_9\", StringType(), True),\n",
    "        StructField(\"end_cell_id_9\", StringType(), True),\n",
    "        StructField(\"start_cell_id_10\", StringType(), True),\n",
    "        StructField(\"end_cell_id_10\", StringType(), True),\n",
    "        StructField(\"delay\", DoubleType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Create the DataFrame using the explicit schema\n",
    "    result_df = spark.createDataFrame([output_row], schema=output_schema)\n",
    "    result_df.show(truncate=False)\n",
    "\n",
    "# Convert the datetime columns\n",
    "streaming_df = streaming_df.withColumn(\"pickup_datetime\", to_timestamp(col(\"pickup_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "streaming_df = streaming_df.withColumn(\"dropoff_datetime\", to_timestamp(col(\"dropoff_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "streaming_df = streaming_df.withColumn(\"ingest_time\", current_timestamp())\n",
    "\n",
    "# Use trigger(once=True) to process existing data exactly one time\n",
    "query = (\n",
    "    streaming_df.writeStream\n",
    "    .trigger(once=True)                # <--- This forces the query to run just once\n",
    "    .foreachBatch(process_batch)\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b5d803",
   "metadata": {},
   "source": [
    "## Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4942a784-1036-49ee-baca-d583019bfd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, floor, concat, lit, current_timestamp, to_timestamp\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, DoubleType, StringType\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "# Define grid constants for the 250m x 250m grid\n",
    "grid_origin_lat = 41.474937\n",
    "grid_origin_lon = -74.913585\n",
    "delta_lat = 0.00225  # 250m in latitude\n",
    "delta_lon = 0.0030   # 250m in longitude\n",
    "\n",
    "def process_batch_query2(batch_df, batch_id):\n",
    "    # Skip empty batches\n",
    "    if batch_df.rdd.isEmpty():\n",
    "        return\n",
    "\n",
    "    # Compute the maximum dropoff time in the current batch.\n",
    "    max_dropoff = batch_df.select(F.max(\"dropoff_datetime\")).first()[0]\n",
    "    if max_dropoff is None:\n",
    "        return\n",
    "\n",
    "    # Reference times:\n",
    "    # For profit: trips with dropoff in the last 15 minutes.\n",
    "    # For empty taxi count: dropoffs in the last 30 minutes.\n",
    "    ref_time_profit = max_dropoff - timedelta(minutes=15)\n",
    "    ref_time_empty  = max_dropoff - timedelta(minutes=30)\n",
    "\n",
    "    # Add cell identifiers for pickup and dropoff\n",
    "    df_with_cells = batch_df.withColumn(\n",
    "        \"pickup_cell\",\n",
    "        concat(\n",
    "            (floor((col(\"pickup_longitude\") - lit(grid_origin_lon)) / lit(delta_lon)) + lit(1)).cast(\"int\"),\n",
    "            lit(\".\"),\n",
    "            (floor((lit(grid_origin_lat) - col(\"pickup_latitude\")) / lit(delta_lat)) + lit(1)).cast(\"int\")\n",
    "        )\n",
    "    ).withColumn(\n",
    "        \"dropoff_cell\",\n",
    "        concat(\n",
    "            (floor((col(\"dropoff_longitude\") - lit(grid_origin_lon)) / lit(delta_lon)) + lit(1)).cast(\"int\"),\n",
    "            lit(\".\"),\n",
    "            (floor((lit(grid_origin_lat) - col(\"dropoff_latitude\")) / lit(delta_lat)) + lit(1)).cast(\"int\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Profit Computation\n",
    "    df_profit = df_with_cells.filter(col(\"dropoff_datetime\") >= F.lit(ref_time_profit)) \\\n",
    "                             .withColumn(\"profit\", col(\"fare_amount\") + col(\"tip_amount\"))\n",
    "    df_profit_grouped = df_profit.groupBy(\"pickup_cell\") \\\n",
    "                                 .agg(F.expr(\"percentile_approx(profit, 0.5)\").alias(\"median_profit\")) \\\n",
    "                                 .withColumnRenamed(\"pickup_cell\", \"cell_id\")\n",
    "\n",
    "    # Empty Taxi Count Computation\n",
    "    windowSpec = Window.partitionBy(\"medallion\").orderBy(\"pickup_datetime\")\n",
    "    df_with_next = df_with_cells.withColumn(\"next_pickup\", F.lead(\"pickup_datetime\").over(windowSpec))\n",
    "    df_empty = df_with_next.filter(\n",
    "        (col(\"dropoff_datetime\") >= F.lit(ref_time_empty)) &\n",
    "        ((col(\"next_pickup\").isNull()) | (col(\"next_pickup\") > col(\"dropoff_datetime\")))\n",
    "    )\n",
    "    df_empty_grouped = df_empty.groupBy(\"dropoff_cell\") \\\n",
    "                               .agg(F.countDistinct(\"medallion\").alias(\"empty_taxi_count\")) \\\n",
    "                               .withColumnRenamed(\"dropoff_cell\", \"cell_id\")\n",
    "\n",
    "    # Profitability Computation\n",
    "    df_area = df_profit_grouped.join(df_empty_grouped, on=\"cell_id\", how=\"outer\") \\\n",
    "                               .withColumn(\n",
    "                                   \"profitability_of_cell\",\n",
    "                                   F.when(col(\"empty_taxi_count\") > 0, col(\"median_profit\") / col(\"empty_taxi_count\")).otherwise(None)\n",
    "                               )\n",
    "    top10_areas = df_area.orderBy(col(\"profitability_of_cell\").desc()).limit(10)\n",
    "    top10_list = top10_areas.collect()\n",
    "\n",
    "    # Print top 10 areas\n",
    "    print(\"10 most profitable areas:\")\n",
    "    for i, area in enumerate(top10_list, start=1):\n",
    "        print(f\"Area {i}:\")\n",
    "        print(f\"  Cell ID: {area['cell_id']}\")\n",
    "        print(f\"  Empty Taxis: {area['empty_taxi_count']}\")\n",
    "        print(f\"  Median Profit: {area['median_profit']}\")\n",
    "        print(f\"  Profitability: {area['profitability_of_cell']}\")\n",
    "    \n",
    "    # Create Output Row for the streaming update.\n",
    "    trigger_row = df_profit.orderBy(col(\"dropoff_datetime\").desc()).limit(1).collect()[0]\n",
    "    trigger_pickup = trigger_row[\"pickup_datetime\"]\n",
    "    trigger_dropoff = trigger_row[\"dropoff_datetime\"]\n",
    "    ingest_time = trigger_row[\"ingest_time\"]\n",
    "    processing_time = datetime.now()\n",
    "    delay = (processing_time - ingest_time).total_seconds()\n",
    "\n",
    "    output_row = {\n",
    "        \"pickup_datetime\": trigger_pickup,\n",
    "        \"dropoff_datetime\": trigger_dropoff,\n",
    "        \"delay\": delay\n",
    "    }\n",
    "    for i in range(10):\n",
    "        if i < len(top10_list):\n",
    "            area = top10_list[i]\n",
    "            output_row[f\"profitable_cell_id_{i+1}\"] = area[\"cell_id\"]\n",
    "            output_row[f\"empty_taxies_in_cell_id_{i+1}\"] = area[\"empty_taxi_count\"]\n",
    "            output_row[f\"median_profit_in_cell_id_{i+1}\"] = area[\"median_profit\"]\n",
    "            output_row[f\"profitability_of_cell_{i+1}\"] = area[\"profitability_of_cell\"]\n",
    "        else:\n",
    "            output_row[f\"profitable_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"empty_taxies_in_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"median_profit_in_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"profitability_of_cell_{i+1}\"] = None\n",
    "\n",
    "    print(f\"Update for batch {batch_id}:\", output_row)\n",
    "\n",
    "    # Define the output schema and create the result df\n",
    "    schema_fields = [\n",
    "        StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "        StructField(\"dropoff_datetime\", TimestampType(), True)\n",
    "    ]\n",
    "    for i in range(10):\n",
    "        schema_fields.extend([\n",
    "            StructField(f\"profitable_cell_id_{i+1}\", StringType(), True),\n",
    "            StructField(f\"empty_taxies_in_cell_id_{i+1}\", StringType(), True),\n",
    "            StructField(f\"median_profit_in_cell_id_{i+1}\", StringType(), True),\n",
    "            StructField(f\"profitability_of_cell_{i+1}\", StringType(), True)\n",
    "        ])\n",
    "    schema_fields.append(StructField(\"delay\", DoubleType(), True))\n",
    "    output_schema = StructType(schema_fields)\n",
    "\n",
    "    result_df = spark.createDataFrame([output_row], schema=output_schema)\n",
    "    result_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c15bbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, current_timestamp\n",
    "\n",
    "new_spark = spark.newSession()\n",
    "taxi_streaming_query2_df = new_spark.readStream.format(\"delta\").load(\"/tmp/delta/taxi_data\")\n",
    "taxi_streaming_query2_df = taxi_streaming_query2_df.withColumn(\"ingest_time\", current_timestamp()) \\\n",
    "    .withColumn(\"pickup_datetime\", to_timestamp(col(\"pickup_datetime\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "    .withColumn(\"dropoff_datetime\", to_timestamp(col(\"dropoff_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "query2 = taxi_streaming_query2_df.writeStream \\\n",
    "    .trigger(once=True) \\\n",
    "    .foreachBatch(process_batch_query2) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "query2.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877f75ba-3043-4d11-a328-c2bb78baf5ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
