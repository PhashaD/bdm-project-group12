{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de047a38-145e-4c9c-b358-97472554bcc4",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b21e607-fcbb-480b-9abe-3f9536cb0b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: delta-spark in /opt/conda/lib/python3.11/site-packages (3.3.0)\n",
      "Requirement already satisfied: pyspark<3.6.0,>=3.5.3 in /usr/local/spark/python (from delta-spark) (3.5.3)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from delta-spark) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata>=1.0.0->delta-spark) (3.20.2)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark<3.6.0,>=3.5.3->delta-spark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install delta-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7cadb3c-ae8f-4cf8-9b74-477eea83109e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "builder = SparkSession.builder.appName(\"project2\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6a9f55-9d79-4a43-a3eb-2a746e9bc4f6",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e203a57-848a-4c19-9e64-74f1fc02d118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- trip_time_in_secs: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- surcharge: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the expected column names based on the CSV's field count\n",
    "columns = [\n",
    "    \"medallion\", \"hack_license\", \"pickup_datetime\", \"dropoff_datetime\",\n",
    "    \"trip_time_in_secs\", \"trip_distance\", \"pickup_longitude\", \"pickup_latitude\",\n",
    "    \"dropoff_longitude\", \"dropoff_latitude\", \"payment_type\", \"fare_amount\",\n",
    "    \"surcharge\", \"mta_tax\", \"tip_amount\", \"tolls_amount\", \"total_amount\"\n",
    "]\n",
    "\n",
    "df = spark.read.option(\"header\", \"false\").csv(\"input/sorted_data.csv\")\n",
    "df = df.toDF(*columns)\n",
    "df.printSchema()\n",
    "\n",
    "fraction = 1.0 / 12  # Approximately 0.03\n",
    "df_sample = df.sample(withReplacement=False, fraction=fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee564ad2-117b-4678-9fc6-79c2b16b8c79",
   "metadata": {},
   "source": [
    "## Data cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a46348fb-cb0f-46c2-8367-d40e5b355a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|           medallion|        hack_license|    pickup_datetime|   dropoff_datetime|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|1920DC2DE67BFC92C...|6728CFBAFBF5F7245...|2013-01-01 00:00:02|2013-01-01 00:03:18|              195|          0.7|      -73.978500|      40.741291|       -73.976997|       40.732452|         CSH|        4.5|      0.5|    0.5|       0.0|         0.0|        5.50|\n",
      "|784131EEAAFACD3B8...|DB805A7306FBF4874...|2013-01-01 00:00:16|2013-01-01 00:04:11|              234|          0.8|      -73.957649|      40.717785|       -73.966522|       40.714657|         CSH|        4.5|      0.5|    0.5|       0.0|         0.0|        5.50|\n",
      "|8402CBED165898F31...|0DB1416B58D5527FD...|2013-01-01 00:02:12|2013-01-01 00:04:29|              137|          0.3|      -73.999390|      40.739437|       -74.000252|       40.742680|         CSH|        3.5|      0.5|    0.5|       0.0|         0.0|        4.50|\n",
      "|226FFD36478408BE1...|E58D07823A60C6361...|2013-01-01 00:00:00|2013-01-01 00:05:00|              300|         0.48|      -73.991821|      40.695587|       -73.990479|       40.700699|         CSH|        5.5|      0.5|    0.5|       0.0|         0.0|        6.50|\n",
      "|5314D17CCFB6C2771...|4DDDB301E37CF29FE...|2013-01-01 00:02:00|2013-01-01 00:05:00|              180|         1.25|      -74.004112|      40.654503|       -73.991791|       40.666534|         CSH|        5.5|      0.5|    0.5|       0.0|         0.0|        6.50|\n",
      "|9483C06260980484F...|5490FCCC1EEFC71E5...|2013-01-01 00:04:15|2013-01-01 00:05:43|               88|          0.5|      -73.967789|      40.768753|       -73.961693|       40.773579|         CSH|        3.5|      0.5|    0.5|       0.0|         0.0|        4.50|\n",
      "|81124DBD6AB03F24B...|EFEDEB0551AA8916D...|2013-01-01 00:05:40|2013-01-01 00:05:49|                9|          0.1|      -73.805092|      40.650558|       -73.806244|       40.652225|         NOC|       20.0|      0.5|    0.0|       0.0|         0.0|       20.50|\n",
      "|AFD2466E58F11165B...|CAD4B59BED1B935BA...|2013-01-01 00:04:00|2013-01-01 00:06:00|              120|         0.98|      -73.954994|      40.769287|       -73.947144|       40.779976|         CSH|        4.5|      0.5|    0.5|       0.0|         0.0|        5.50|\n",
      "|D9598D121715B456C...|34887B903219BC5FE...|2013-01-01 00:01:00|2013-01-01 00:06:00|              300|         1.04|      -73.981918|      40.771168|       -73.969978|       40.767078|         CSH|        6.0|      0.5|    0.5|       0.0|         0.0|        7.00|\n",
      "|1C0621B8B81EEC5FC...|AA38851431A0EC592...|2013-01-01 00:01:12|2013-01-01 00:06:13|              301|          0.9|      -73.977341|      40.751785|       -73.987991|       40.743916|         CRD|        5.5|      0.5|    0.5|       1.3|         0.0|        7.80|\n",
      "|1C341430AF45F688E...|9F7118CDF1876A0E4...|2013-01-01 00:05:01|2013-01-01 00:06:36|               94|          0.2|      -74.005356|      40.720814|       -74.004868|       40.723621|         CSH|        3.0|      0.5|    0.5|       0.0|         0.0|        4.00|\n",
      "|02E271D8CEBB28463...|69365235F0D552B41...|2013-01-01 00:02:40|2013-01-01 00:06:45|              245|          0.5|      -73.989342|      40.757679|       -73.983727|       40.765575|         CSH|        4.5|      0.5|    0.5|       0.0|         0.0|        5.50|\n",
      "|A8BB4EED2F83D09D0...|35E41611697EA9025...|2013-01-01 00:00:33|2013-01-01 00:07:22|              408|          1.5|      -73.961060|      40.765018|       -73.978745|       40.752563|         CRD|        7.5|      0.5|    0.5|      2.12|         0.0|       10.62|\n",
      "|681E7CCDEF7666DF0...|7DBAA1EACBC91DCA9...|2013-01-01 00:05:00|2013-01-01 00:08:00|              180|         1.12|      -73.955864|      40.778851|       -73.964111|       40.767792|         CSH|        5.0|      0.5|    0.5|       0.0|         0.0|        6.00|\n",
      "|6B129E331EDE126B4...|79D0A7B682ABC516B...|2013-01-01 00:00:00|2013-01-01 00:08:00|              480|         2.73|      -73.973907|      40.747868|       -73.948898|       40.776649|         CRD|        9.5|      0.5|    0.5|       2.0|         0.0|       12.50|\n",
      "|3909E17FC037EF50A...|1A15E27E93EEBDDBE...|2013-01-01 00:06:01|2013-01-01 00:08:25|              143|          0.1|      -73.997536|      40.760487|       -74.000069|       40.761585|         CSH|        3.5|      0.5|    0.5|       0.0|         0.0|        4.50|\n",
      "|1E26FB273E08FC0BE...|88C76ADA068ED66B7...|2013-01-01 00:06:33|2013-01-01 00:08:28|              113|          0.9|        0.000000|       0.000000|         0.000000|        0.000000|         CSH|        4.5|      0.5|    0.5|       0.0|         0.0|        5.50|\n",
      "|17128351740DC1755...|4B263C1CC4B5900E9...|2013-01-01 00:01:00|2013-01-01 00:09:00|              480|         0.88|      -73.984154|      40.748501|       -73.978378|       40.753822|         CSH|        7.0|      0.5|    0.5|       0.0|         0.0|        8.00|\n",
      "|90B11B457EE0FB2D9...|7E458DFE39A1B9DF6...|2013-01-01 00:07:00|2013-01-01 00:09:00|              120|         0.46|      -73.985207|      40.718807|       -73.985207|       40.718807|         CSH|        3.5|      0.5|    0.5|       0.0|         0.0|        4.50|\n",
      "|A105D655494EF46AA...|C4A1213ABE84E943D...|2013-01-01 00:06:00|2013-01-01 00:09:00|              180|         0.72|      -73.939499|      40.847797|       -73.943138|       40.839931|         CSH|        4.5|      0.5|    0.5|       0.0|         0.0|        5.50|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "# Convert pickup and dropoff datetimes to timestamps\n",
    "df_sample = df_sample.withColumn(\"pickup_datetime\", to_timestamp(col(\"pickup_datetime\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "                     .withColumn(\"dropoff_datetime\", to_timestamp(col(\"dropoff_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "clean_df = df_sample.filter(\n",
    "    (col(\"pickup_datetime\").isNotNull()) &\n",
    "    (col(\"dropoff_datetime\").isNotNull()) &\n",
    "    (col(\"medallion\").isNotNull()) & (col(\"medallion\") != \"\") &\n",
    "    (col(\"hack_license\").isNotNull()) & (col(\"hack_license\") != \"\") &\n",
    "    (col(\"trip_distance\").cast(\"float\") > 0) &\n",
    "    (col(\"fare_amount\").cast(\"float\") > 0)\n",
    ")\n",
    "\n",
    "clean_df = clean_df.withColumn(\"trip_time_in_secs\", col(\"trip_time_in_secs\").cast(\"int\")) \\\n",
    "                   .withColumn(\"trip_distance\", col(\"trip_distance\").cast(\"float\")) \\\n",
    "                   .withColumn(\"fare_amount\", col(\"fare_amount\").cast(\"float\")) \\\n",
    "                   .withColumn(\"surcharge\", col(\"surcharge\").cast(\"float\")) \\\n",
    "                   .withColumn(\"mta_tax\", col(\"mta_tax\").cast(\"float\")) \\\n",
    "                   .withColumn(\"tip_amount\", col(\"tip_amount\").cast(\"float\")) \\\n",
    "                   .withColumn(\"tolls_amount\", col(\"tolls_amount\").cast(\"float\"))\n",
    "clean_df.count()\n",
    "clean_df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54a3e5f-f084-43bb-8956-6f7f33b97772",
   "metadata": {},
   "source": [
    "## Query 1 - Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11319598-cc2a-48ea-99e1-865c041e8aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max dropoff datetime: 2014-01-01 00:37:00\n",
      "+----------+----------+---------------+\n",
      "|start_cell|end_cell  |Number_of_Rides|\n",
      "+----------+----------+---------------+\n",
      "|4073_-7399|4075_-7400|2              |\n",
      "|4076_-7399|4075_-7400|2              |\n",
      "|4076_-7399|4072_-7399|2              |\n",
      "|4071_-7401|4069_-7386|1              |\n",
      "|4075_-7397|4079_-7398|1              |\n",
      "|4067_-7396|4072_-7395|1              |\n",
      "|4081_-7392|4083_-7385|1              |\n",
      "|4076_-7398|4075_-7400|1              |\n",
      "|4064_-7378|4061_-7392|1              |\n",
      "|4069_-7394|4074_-7387|1              |\n",
      "+----------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, floor, concat, lit, current_timestamp, expr\n",
    "from datetime import timedelta\n",
    "\n",
    "max_dropoff = clean_df.agg({\"dropoff_datetime\": \"max\"}).collect()[0][0]\n",
    "print(\"Max dropoff datetime:\", max_dropoff)\n",
    "\n",
    "ref_time = max_dropoff - timedelta(minutes=30)\n",
    "\n",
    "# Create grid cell identifiers for pickup (start_cell) and drop-off (end_cell)\n",
    "cell_size = 0.01\n",
    "\n",
    "df_routes = clean_df.withColumn(\n",
    "    \"start_cell\",\n",
    "    concat(\n",
    "        floor(col(\"pickup_latitude\") / cell_size),\n",
    "        lit(\"_\"),\n",
    "        floor(col(\"pickup_longitude\") / cell_size)\n",
    "    )\n",
    ").withColumn(\n",
    "    \"end_cell\",\n",
    "    concat(\n",
    "        floor(col(\"dropoff_latitude\") / cell_size),\n",
    "        lit(\"_\"),\n",
    "        floor(col(\"dropoff_longitude\") / cell_size)\n",
    "    )\n",
    ")\n",
    "\n",
    "df_last30 = df_routes.filter(col(\"dropoff_datetime\") >= lit(ref_time))\n",
    "\n",
    "\n",
    "# Group by start and end cell and count the rides\n",
    "df_frequent_routes = df_last30.groupBy(\"start_cell\", \"end_cell\").count() \\\n",
    "    .withColumnRenamed(\"count\", \"Number_of_Rides\")\n",
    "\n",
    "top10_routes = df_frequent_routes.orderBy(col(\"Number_of_Rides\").desc()).limit(10)\n",
    "top10_routes.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aa4abb-f52a-40b4-b71e-c37c552a757f",
   "metadata": {},
   "source": [
    "## Query 1 - Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16784a00-324c-4192-bc63-3ee8bf0a1076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the cleansed data to a Delta table in a writable directory.\n",
    "clean_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta/taxi_data\")\n",
    "streaming_df = spark.readStream.format(\"delta\").load(\"/tmp/delta/taxi_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fe1282a-f816-4eba-b092-a17b31f5bd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- trip_time_in_secs: integer (nullable = true)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: float (nullable = true)\n",
      " |-- surcharge: float (nullable = true)\n",
      " |-- mta_tax: float (nullable = true)\n",
      " |-- tip_amount: float (nullable = true)\n",
      " |-- tolls_amount: float (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      "\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|medallion                       |hack_license                    |pickup_datetime    |dropoff_datetime   |trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|8F765363C77E091F4F364858A370AE9C|202CC839A8CCC598FB7BE5FB52159E43|2013-02-02 00:48:00|2013-02-02 01:00:00|720              |2.84         |-74.001350      |40.735992      |-73.971779       |40.758003       |CSH         |11.5       |0.5      |0.5    |0.0       |0.0         |12.50       |\n",
      "|901BAD071B1C693D943B2BAA3FE6E623|4FBF0FE605244EBB55F7139279A1D6BE|2013-02-02 00:57:00|2013-02-02 01:00:00|180              |0.85         |-73.984024      |40.746418      |-73.974541       |40.750763       |CSH         |4.5        |0.5      |0.5    |0.0       |0.0         |5.50        |\n",
      "|A506888A914B08DDA4C1FD8649BA3C28|7961BA824986F5B968565C4D894A5074|2013-02-02 00:56:00|2013-02-02 01:00:00|240              |0.38         |-73.993965      |40.717880      |-73.993965       |40.717880       |CRD         |4.5        |0.5      |0.5    |1.25      |0.0         |6.75        |\n",
      "|B458C7D184125F689833D71A5A648B76|FC1A705DA838C6B82B123D37B91C365E|2013-02-02 00:57:00|2013-02-02 01:00:00|180              |1.35         |-73.953560      |40.775311      |-73.941643       |40.787575       |CSH         |5.5        |0.5      |0.5    |0.0       |0.0         |6.50        |\n",
      "|BE386D8524FCD16B3727DCF0A32D9B25|4EB96EC9F3A42794DEE233EC8A2616CE|2013-02-02 00:45:00|2013-02-02 01:00:00|900              |2.8          |-73.985504      |40.731709      |-73.987419       |40.760769       |CRD         |12.5       |0.5      |0.5    |2.6       |0.0         |16.10       |\n",
      "+--------------------------------+--------------------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Row count: 14334468\n"
     ]
    }
   ],
   "source": [
    "# 1) Read the Delta table in batch mode\n",
    "check_df = spark.read.format(\"delta\").load(\"/tmp/delta/taxi_data\")\n",
    "\n",
    "# 2) See the schema\n",
    "check_df.printSchema()\n",
    "\n",
    "# 3) Inspect some rows\n",
    "check_df.show(5, truncate=False)\n",
    "\n",
    "# 4) Count the rows\n",
    "print(\"Row count:\", check_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05031d45-276b-4917-a94d-1205c449b158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window filter: dropoff_datetime >= 2014-01-01 00:32:18\n",
      "df_last30 count = 3\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+--------------------+----------------+-----------------+----------+-----------------+------------------+--------+\n",
      "|           medallion|        hack_license|    pickup_datetime|   dropoff_datetime|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total_amount|         ingest_time|pickup_cell_east|pickup_cell_south|start_cell|dropoff_cell_east|dropoff_cell_south|end_cell|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+--------------------+----------------+-----------------+----------+-----------------+------------------+--------+\n",
      "|FE6C2621EAF58A69A...|97191A64A3C40CFD2...|2013-12-31 23:34:00|2014-01-01 00:42:00|             4080|         7.01|      -73.997566|      40.763096|       -73.926826|       40.743900|         CSH|       44.0|      0.5|    0.5|       0.0|         0.0|       45.00|2025-03-19 22:24:...|             153|              159|   153.159|              165|               163| 165.163|\n",
      "|B42FC03D269AA4FBA...|106116F0B19797E41...|2013-12-31 23:57:36|2014-01-01 00:51:19|             3222|          7.9|      -74.004570|      40.721470|       -73.959282|       40.811058|         CRD|       36.0|      0.5|    0.5|      9.25|         0.0|       46.25|2025-03-19 22:24:...|             152|              168|   152.168|              160|               148| 160.148|\n",
      "|F7E6BF13B6A8E8FD0...|6AC6033E0F915C291...|2013-12-31 22:45:43|2014-01-01 01:02:18|             8195|         12.5|      -73.974464|      40.761929|       -73.988609|       40.693043|         CSH|       88.5|      0.5|    0.5|       0.0|         0.0|       89.50|2025-03-19 22:24:...|             157|              159|   157.159|              155|               174| 155.174|\n",
      "+--------------------+--------------------+-------------------+-------------------+-----------------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+--------------------+----------------+-----------------+----------+-----------------+------------------+--------+\n",
      "\n",
      "Update for batch 0 : {'pickup_datetime': datetime.datetime(2013, 12, 31, 22, 45, 43), 'dropoff_datetime': datetime.datetime(2014, 1, 1, 1, 2, 18), 'delay': 31.757094, 'start_cell_id_1': '157.159', 'end_cell_id_1': '155.174', 'start_cell_id_2': '152.168', 'end_cell_id_2': '160.148', 'start_cell_id_3': '153.159', 'end_cell_id_3': '165.163', 'start_cell_id_4': None, 'end_cell_id_4': None, 'start_cell_id_5': None, 'end_cell_id_5': None, 'start_cell_id_6': None, 'end_cell_id_6': None, 'start_cell_id_7': None, 'end_cell_id_7': None, 'start_cell_id_8': None, 'end_cell_id_8': None, 'start_cell_id_9': None, 'end_cell_id_9': None, 'start_cell_id_10': None, 'end_cell_id_10': None}\n",
      "+-------------------+-------------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+----------------+--------------+---------+\n",
      "|pickup_datetime    |dropoff_datetime   |start_cell_id_1|end_cell_id_1|start_cell_id_2|end_cell_id_2|start_cell_id_3|end_cell_id_3|start_cell_id_4|end_cell_id_4|start_cell_id_5|end_cell_id_5|start_cell_id_6|end_cell_id_6|start_cell_id_7|end_cell_id_7|start_cell_id_8|end_cell_id_8|start_cell_id_9|end_cell_id_9|start_cell_id_10|end_cell_id_10|delay    |\n",
      "+-------------------+-------------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+----------------+--------------+---------+\n",
      "|2013-12-31 22:45:43|2014-01-01 01:02:18|157.159        |155.174      |152.168        |160.148      |153.159        |165.163      |NULL           |NULL         |NULL           |NULL         |NULL           |NULL         |NULL           |NULL         |NULL           |NULL         |NULL           |NULL         |NULL            |NULL          |31.757094|\n",
      "+-------------------+-------------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+---------------+-------------+----------------+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, floor, concat, lit, current_timestamp, to_timestamp, expr\n",
    ")\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import timedelta, datetime\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, DoubleType, StringType\n",
    "\n",
    "# Define your grid constants for the 500m x 500m grid\n",
    "grid_origin_lat = 41.474937\n",
    "grid_origin_lon = -74.913585\n",
    "delta_lat = 0.0045   # Approximate degrees for 500m in latitude\n",
    "delta_lon = 0.0060   # Approximate degrees for 500m in longitude\n",
    "\n",
    "# This is your foreachBatch function to process each micro-batch\n",
    "def process_batch(batch_df, batch_id):\n",
    "    # Skip empty batches\n",
    "    if batch_df.rdd.isEmpty():\n",
    "        return\n",
    "\n",
    "    # PART A: Compute the 30-minute window based on the batch’s max dropoff\n",
    "    max_dropoff = batch_df.agg({\"dropoff_datetime\": \"max\"}).collect()[0][0]\n",
    "    if max_dropoff is None:\n",
    "        return\n",
    "    ref_time = max_dropoff - timedelta(minutes=30)\n",
    "\n",
    "    # PART B: Compute grid cell IDs for pickup and dropoff using the 500m grid\n",
    "    batch_df = batch_df.withColumn(\n",
    "        \"pickup_cell_east\", floor((col(\"pickup_longitude\") - lit(grid_origin_lon)) / lit(delta_lon)) + 1\n",
    "    ).withColumn(\n",
    "        \"pickup_cell_south\", floor((lit(grid_origin_lat) - col(\"pickup_latitude\")) / lit(delta_lat)) + 1\n",
    "    ).withColumn(\n",
    "        \"start_cell\", concat(col(\"pickup_cell_east\").cast(\"int\"), lit(\".\"), col(\"pickup_cell_south\").cast(\"int\"))\n",
    "    )\n",
    "    batch_df = batch_df.withColumn(\n",
    "        \"dropoff_cell_east\", floor((col(\"dropoff_longitude\") - lit(grid_origin_lon)) / lit(delta_lon)) + 1\n",
    "    ).withColumn(\n",
    "        \"dropoff_cell_south\", floor((lit(grid_origin_lat) - col(\"dropoff_latitude\")) / lit(delta_lat)) + 1\n",
    "    ).withColumn(\n",
    "        \"end_cell\", concat(col(\"dropoff_cell_east\").cast(\"int\"), lit(\".\"), col(\"dropoff_cell_south\").cast(\"int\"))\n",
    "    )\n",
    "\n",
    "    # Filter out trips that are out-of-bounds (only consider cells 1 to 300)\n",
    "    batch_df = batch_df.filter(\n",
    "        (col(\"pickup_cell_east\").between(1, 300)) &\n",
    "        (col(\"pickup_cell_south\").between(1, 300)) &\n",
    "        (col(\"dropoff_cell_east\").between(1, 300)) &\n",
    "        (col(\"dropoff_cell_south\").between(1, 300))\n",
    "    )\n",
    "\n",
    "    # PART C: Filter for trips with dropoff_datetime >= ref_time (last 30 minutes)\n",
    "    df_last30 = batch_df.filter(col(\"dropoff_datetime\") >= F.lit(ref_time))\n",
    "    print(f\"Window filter: dropoff_datetime >= {ref_time}\")\n",
    "    print(\"df_last30 count =\", df_last30.count())\n",
    "    df_last30.show(5)\n",
    "\n",
    "\n",
    "    # PART D: Aggregate routes and get top 10 most frequent\n",
    "    df_frequent_routes = df_last30.groupBy(\"start_cell\", \"end_cell\") \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed(\"count\", \"Number_of_Rides\")\n",
    "    top10_routes = df_frequent_routes.orderBy(col(\"Number_of_Rides\").desc()).limit(10)\n",
    "    top10_list = top10_routes.collect()\n",
    "\n",
    "    # PART E: Determine a triggering event and compute delay\n",
    "    # Choose the event with the maximum dropoff_datetime as the trigger\n",
    "    trigger_row = batch_df.orderBy(col(\"dropoff_datetime\").desc()).limit(1).collect()[0]\n",
    "    trigger_pickup = trigger_row[\"pickup_datetime\"]\n",
    "    trigger_dropoff = trigger_row[\"dropoff_datetime\"]\n",
    "    ingest_time = trigger_row[\"ingest_time\"]\n",
    "    processing_time = datetime.now()\n",
    "    delay = (processing_time - ingest_time).total_seconds()\n",
    "\n",
    "    # PART F: Build the output row\n",
    "    output_row = {\n",
    "        \"pickup_datetime\": trigger_pickup,\n",
    "        \"dropoff_datetime\": trigger_dropoff,\n",
    "        \"delay\": delay\n",
    "    }\n",
    "    for i in range(10):\n",
    "        if i < len(top10_list):\n",
    "            route = top10_list[i]\n",
    "            output_row[f\"start_cell_id_{i+1}\"] = route[\"start_cell\"]\n",
    "            output_row[f\"end_cell_id_{i+1}\"] = route[\"end_cell\"]\n",
    "        else:\n",
    "            output_row[f\"start_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"end_cell_id_{i+1}\"] = None\n",
    "\n",
    "    # For demonstration, print the output update\n",
    "    print(f\"Update for batch {batch_id} :\", output_row)\n",
    "    \n",
    "    # Define the output schema explicitly\n",
    "    output_schema = StructType([\n",
    "        StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "        StructField(\"dropoff_datetime\", TimestampType(), True),\n",
    "        StructField(\"start_cell_id_1\", StringType(), True),\n",
    "        StructField(\"end_cell_id_1\", StringType(), True),\n",
    "        StructField(\"start_cell_id_2\", StringType(), True),\n",
    "        StructField(\"end_cell_id_2\", StringType(), True),\n",
    "        StructField(\"start_cell_id_3\", StringType(), True),\n",
    "        StructField(\"end_cell_id_3\", StringType(), True),\n",
    "        StructField(\"start_cell_id_4\", StringType(), True),\n",
    "        StructField(\"end_cell_id_4\", StringType(), True),\n",
    "        StructField(\"start_cell_id_5\", StringType(), True),\n",
    "        StructField(\"end_cell_id_5\", StringType(), True),\n",
    "        StructField(\"start_cell_id_6\", StringType(), True),\n",
    "        StructField(\"end_cell_id_6\", StringType(), True),\n",
    "        StructField(\"start_cell_id_7\", StringType(), True),\n",
    "        StructField(\"end_cell_id_7\", StringType(), True),\n",
    "        StructField(\"start_cell_id_8\", StringType(), True),\n",
    "        StructField(\"end_cell_id_8\", StringType(), True),\n",
    "        StructField(\"start_cell_id_9\", StringType(), True),\n",
    "        StructField(\"end_cell_id_9\", StringType(), True),\n",
    "        StructField(\"start_cell_id_10\", StringType(), True),\n",
    "        StructField(\"end_cell_id_10\", StringType(), True),\n",
    "        StructField(\"delay\", DoubleType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Create the DataFrame using the explicit schema\n",
    "    result_df = spark.createDataFrame([output_row], schema=output_schema)\n",
    "    result_df.show(truncate=False)\n",
    "\n",
    "# If your taxi data doesn’t already have proper types, ensure you convert the datetime columns\n",
    "streaming_df = streaming_df.withColumn(\"pickup_datetime\", to_timestamp(col(\"pickup_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "streaming_df = streaming_df.withColumn(\"dropoff_datetime\", to_timestamp(col(\"dropoff_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "streaming_df = streaming_df.withColumn(\"ingest_time\", current_timestamp())\n",
    "\n",
    "# Use trigger(once=True) to process existing data exactly one time\n",
    "query = (\n",
    "    streaming_df.writeStream\n",
    "    .trigger(once=True)                # <--- This forces the query to run just once\n",
    "    .foreachBatch(process_batch)\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b5d803",
   "metadata": {},
   "source": [
    "## Query 2 - part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4942a784-1036-49ee-baca-d583019bfd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, floor, concat, lit, current_timestamp, to_timestamp\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, DoubleType, StringType\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "# Define grid constants for the 500m x 500m grid\n",
    "grid_origin_lat = 41.474937\n",
    "grid_origin_lon = -74.913585\n",
    "delta_lat = 0.0045\n",
    "delta_lon = 0.0060\n",
    "\n",
    "def process_batch_query2(batch_df, batch_id):\n",
    "    # Skip empty batches\n",
    "    if batch_df.rdd.isEmpty():\n",
    "        return\n",
    "\n",
    "    # Compute the maximum dropoff time in the current batch.\n",
    "    max_dropoff = batch_df.agg(F.max(\"dropoff_datetime\")).collect()[0][0]\n",
    "    if max_dropoff is None:\n",
    "        return\n",
    "\n",
    "    # Define reference times:\n",
    "    # For profit: trips with dropoff in the last 15 minutes.\n",
    "    # For empty taxi count: dropoffs in the last 30 minutes.\n",
    "    ref_time_profit = max_dropoff - timedelta(minutes=15)\n",
    "    ref_time_empty  = max_dropoff - timedelta(minutes=30)\n",
    "\n",
    "    # STEP 1: Add grid cell identifiers for pickup and dropoff.\n",
    "    df_with_cells = batch_df.withColumn(\n",
    "        \"pickup_cell\",\n",
    "        concat(\n",
    "            (floor((col(\"pickup_longitude\") - lit(grid_origin_lon)) / lit(delta_lon)) + lit(1)).cast(\"int\"),\n",
    "            lit(\".\"),\n",
    "            (floor((lit(grid_origin_lat) - col(\"pickup_latitude\")) / lit(delta_lat)) + lit(1)).cast(\"int\")\n",
    "        )\n",
    "    ).withColumn(\n",
    "        \"dropoff_cell\",\n",
    "        concat(\n",
    "            (floor((col(\"dropoff_longitude\") - lit(grid_origin_lon)) / lit(delta_lon)) + lit(1)).cast(\"int\"),\n",
    "            lit(\".\"),\n",
    "            (floor((lit(grid_origin_lat) - col(\"dropoff_latitude\")) / lit(delta_lat)) + lit(1)).cast(\"int\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # STEP 2: Profit Computation\n",
    "    df_profit = df_with_cells.filter(col(\"dropoff_datetime\") >= F.lit(ref_time_profit)) \\\n",
    "                             .withColumn(\"profit\", col(\"fare_amount\") + col(\"tip_amount\"))\n",
    "    df_profit_grouped = df_profit.groupBy(\"pickup_cell\") \\\n",
    "                                 .agg(F.expr(\"percentile_approx(profit, 0.5)\").alias(\"median_profit\")) \\\n",
    "                                 .withColumnRenamed(\"pickup_cell\", \"cell_id\")\n",
    "\n",
    "    # STEP 3: Empty Taxi Count Computation\n",
    "    windowSpec = Window.partitionBy(\"medallion\").orderBy(\"pickup_datetime\")\n",
    "    df_with_next = df_with_cells.withColumn(\"next_pickup\", F.lead(\"pickup_datetime\").over(windowSpec))\n",
    "    df_empty = df_with_next.filter(\n",
    "        (col(\"dropoff_datetime\") >= F.lit(ref_time_empty)) &\n",
    "        ((col(\"next_pickup\").isNull()) | (col(\"next_pickup\") > col(\"dropoff_datetime\")))\n",
    "    )\n",
    "    df_empty_grouped = df_empty.groupBy(\"dropoff_cell\") \\\n",
    "                               .agg(F.countDistinct(\"medallion\").alias(\"empty_taxi_count\")) \\\n",
    "                               .withColumnRenamed(\"dropoff_cell\", \"cell_id\")\n",
    "\n",
    "    # STEP 4: Compute Profitability\n",
    "    df_area = df_profit_grouped.join(df_empty_grouped, on=\"cell_id\", how=\"outer\") \\\n",
    "                               .withColumn(\n",
    "                                   \"profitability_of_cell\",\n",
    "                                   F.when(col(\"empty_taxi_count\") > 0, col(\"median_profit\") / col(\"empty_taxi_count\")).otherwise(None)\n",
    "                               )\n",
    "    top10_areas = df_area.orderBy(col(\"profitability_of_cell\").desc()).limit(10)\n",
    "    top10_list = top10_areas.collect()\n",
    "\n",
    "    # STEP 5: Print a human-friendly summary of the 10 most profitable areas.\n",
    "    print(\"10 most profitable areas:\")\n",
    "    for i, area in enumerate(top10_list, start=1):\n",
    "        print(f\"Area {i}:\")\n",
    "        print(f\"  Cell ID: {area['cell_id']}\")\n",
    "        print(f\"  Empty Taxis: {area['empty_taxi_count']}\")\n",
    "        print(f\"  Median Profit: {area['median_profit']}\")\n",
    "        print(f\"  Profitability: {area['profitability_of_cell']}\")\n",
    "    \n",
    "    # STEP 6: Create Output Row for the streaming update.\n",
    "    trigger_row = batch_df.orderBy(col(\"dropoff_datetime\").desc()).limit(1).collect()[0]\n",
    "    trigger_pickup = trigger_row[\"pickup_datetime\"]\n",
    "    trigger_dropoff = trigger_row[\"dropoff_datetime\"]\n",
    "    ingest_time = trigger_row[\"ingest_time\"]\n",
    "    processing_time = datetime.now()\n",
    "    delay = (processing_time - ingest_time).total_seconds()\n",
    "\n",
    "    output_row = {\n",
    "        \"pickup_datetime\": trigger_pickup,\n",
    "        \"dropoff_datetime\": trigger_dropoff,\n",
    "        \"delay\": delay\n",
    "    }\n",
    "    for i in range(10):\n",
    "        if i < len(top10_list):\n",
    "            area = top10_list[i]\n",
    "            output_row[f\"profitable_cell_id_{i+1}\"] = area[\"cell_id\"]\n",
    "            output_row[f\"empty_taxies_in_cell_id_{i+1}\"] = area[\"empty_taxi_count\"]\n",
    "            output_row[f\"median_profit_in_cell_id_{i+1}\"] = area[\"median_profit\"]\n",
    "            output_row[f\"profitability_of_cell_{i+1}\"] = area[\"profitability_of_cell\"]\n",
    "        else:\n",
    "            output_row[f\"profitable_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"empty_taxies_in_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"median_profit_in_cell_id_{i+1}\"] = None\n",
    "            output_row[f\"profitability_of_cell_{i+1}\"] = None\n",
    "\n",
    "    print(f\"Update for batch {batch_id}:\", output_row)\n",
    "\n",
    "    # STEP 7: Define the output schema and create the result DataFrame.\n",
    "    schema_fields = [\n",
    "        StructField(\"pickup_datetime\", TimestampType(), True),\n",
    "        StructField(\"dropoff_datetime\", TimestampType(), True)\n",
    "    ]\n",
    "    for i in range(10):\n",
    "        schema_fields.extend([\n",
    "            StructField(f\"profitable_cell_id_{i+1}\", StringType(), True),\n",
    "            StructField(f\"empty_taxies_in_cell_id_{i+1}\", StringType(), True),\n",
    "            StructField(f\"median_profit_in_cell_id_{i+1}\", StringType(), True),\n",
    "            StructField(f\"profitability_of_cell_{i+1}\", StringType(), True)\n",
    "        ])\n",
    "    schema_fields.append(StructField(\"delay\", DoubleType(), True))\n",
    "    output_schema = StructType(schema_fields)\n",
    "\n",
    "    result_df = spark.createDataFrame([output_row], schema=output_schema)\n",
    "    result_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c15bbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most profitable areas:\n",
      "Area 1:\n",
      "  Cell ID: 188.184\n",
      "  Empty Taxis: 1\n",
      "  Median Profit: 52.0\n",
      "  Profitability: 52.0\n",
      "Area 2:\n",
      "  Cell ID: 176.157\n",
      "  Empty Taxis: 1\n",
      "  Median Profit: 34.79999923706055\n",
      "  Profitability: 34.79999923706055\n",
      "Area 3:\n",
      "  Cell ID: 188.185\n",
      "  Empty Taxis: 2\n",
      "  Median Profit: 52.0\n",
      "  Profitability: 26.0\n",
      "Area 4:\n",
      "  Cell ID: 188.186\n",
      "  Empty Taxis: 2\n",
      "  Median Profit: 52.0\n",
      "  Profitability: 26.0\n",
      "Area 5:\n",
      "  Cell ID: 157.151\n",
      "  Empty Taxis: 1\n",
      "  Median Profit: 21.5\n",
      "  Profitability: 21.5\n",
      "Area 6:\n",
      "  Cell ID: 160.153\n",
      "  Empty Taxis: 1\n",
      "  Median Profit: 21.0\n",
      "  Profitability: 21.0\n",
      "Area 7:\n",
      "  Cell ID: 151.170\n",
      "  Empty Taxis: 2\n",
      "  Median Profit: 28.299999237060547\n",
      "  Profitability: 14.149999618530273\n",
      "Area 8:\n",
      "  Cell ID: 163.151\n",
      "  Empty Taxis: 1\n",
      "  Median Profit: 10.0\n",
      "  Profitability: 10.0\n",
      "Area 9:\n",
      "  Cell ID: 153.174\n",
      "  Empty Taxis: 1\n",
      "  Median Profit: 9.5\n",
      "  Profitability: 9.5\n",
      "Area 10:\n",
      "  Cell ID: 152.161\n",
      "  Empty Taxis: 1\n",
      "  Median Profit: 9.300000190734863\n",
      "  Profitability: 9.300000190734863\n",
      "Update for batch 0: {'pickup_datetime': datetime.datetime(2013, 1, 23, 16, 7, 41), 'dropoff_datetime': datetime.datetime(2013, 1, 23, 16, 15, 51), 'delay': 7.44266, 'profitable_cell_id_1': '188.184', 'empty_taxies_in_cell_id_1': 1, 'median_profit_in_cell_id_1': 52.0, 'profitability_of_cell_1': 52.0, 'profitable_cell_id_2': '176.157', 'empty_taxies_in_cell_id_2': 1, 'median_profit_in_cell_id_2': 34.79999923706055, 'profitability_of_cell_2': 34.79999923706055, 'profitable_cell_id_3': '188.185', 'empty_taxies_in_cell_id_3': 2, 'median_profit_in_cell_id_3': 52.0, 'profitability_of_cell_3': 26.0, 'profitable_cell_id_4': '188.186', 'empty_taxies_in_cell_id_4': 2, 'median_profit_in_cell_id_4': 52.0, 'profitability_of_cell_4': 26.0, 'profitable_cell_id_5': '157.151', 'empty_taxies_in_cell_id_5': 1, 'median_profit_in_cell_id_5': 21.5, 'profitability_of_cell_5': 21.5, 'profitable_cell_id_6': '160.153', 'empty_taxies_in_cell_id_6': 1, 'median_profit_in_cell_id_6': 21.0, 'profitability_of_cell_6': 21.0, 'profitable_cell_id_7': '151.170', 'empty_taxies_in_cell_id_7': 2, 'median_profit_in_cell_id_7': 28.299999237060547, 'profitability_of_cell_7': 14.149999618530273, 'profitable_cell_id_8': '163.151', 'empty_taxies_in_cell_id_8': 1, 'median_profit_in_cell_id_8': 10.0, 'profitability_of_cell_8': 10.0, 'profitable_cell_id_9': '153.174', 'empty_taxies_in_cell_id_9': 1, 'median_profit_in_cell_id_9': 9.5, 'profitability_of_cell_9': 9.5, 'profitable_cell_id_10': '152.161', 'empty_taxies_in_cell_id_10': 1, 'median_profit_in_cell_id_10': 9.300000190734863, 'profitability_of_cell_10': 9.300000190734863}\n",
      "+-------------------+-------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+---------------------+--------------------------+---------------------------+------------------------+-------+\n",
      "|pickup_datetime    |dropoff_datetime   |profitable_cell_id_1|empty_taxies_in_cell_id_1|median_profit_in_cell_id_1|profitability_of_cell_1|profitable_cell_id_2|empty_taxies_in_cell_id_2|median_profit_in_cell_id_2|profitability_of_cell_2|profitable_cell_id_3|empty_taxies_in_cell_id_3|median_profit_in_cell_id_3|profitability_of_cell_3|profitable_cell_id_4|empty_taxies_in_cell_id_4|median_profit_in_cell_id_4|profitability_of_cell_4|profitable_cell_id_5|empty_taxies_in_cell_id_5|median_profit_in_cell_id_5|profitability_of_cell_5|profitable_cell_id_6|empty_taxies_in_cell_id_6|median_profit_in_cell_id_6|profitability_of_cell_6|profitable_cell_id_7|empty_taxies_in_cell_id_7|median_profit_in_cell_id_7|profitability_of_cell_7|profitable_cell_id_8|empty_taxies_in_cell_id_8|median_profit_in_cell_id_8|profitability_of_cell_8|profitable_cell_id_9|empty_taxies_in_cell_id_9|median_profit_in_cell_id_9|profitability_of_cell_9|profitable_cell_id_10|empty_taxies_in_cell_id_10|median_profit_in_cell_id_10|profitability_of_cell_10|delay  |\n",
      "+-------------------+-------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+---------------------+--------------------------+---------------------------+------------------------+-------+\n",
      "|2013-01-23 16:07:41|2013-01-23 16:15:51|188.184             |1                        |52.0                      |52.0                   |176.157             |1                        |34.79999923706055         |34.79999923706055      |188.185             |2                        |52.0                      |26.0                   |188.186             |2                        |52.0                      |26.0                   |157.151             |1                        |21.5                      |21.5                   |160.153             |1                        |21.0                      |21.0                   |151.170             |2                        |28.299999237060547        |14.149999618530273     |163.151             |1                        |10.0                      |10.0                   |153.174             |1                        |9.5                       |9.5                    |152.161              |1                         |9.300000190734863          |9.300000190734863       |7.44266|\n",
      "+-------------------+-------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+--------------------+-------------------------+--------------------------+-----------------------+---------------------+--------------------------+---------------------------+------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, current_timestamp\n",
    "\n",
    "new_spark = spark.newSession()\n",
    "taxi_streaming_query2_df = new_spark.readStream.format(\"delta\").load(\"/tmp/delta/taxi_data\")\n",
    "taxi_streaming_query2_df = taxi_streaming_query2_df.withColumn(\"ingest_time\", current_timestamp()) \\\n",
    "    .withColumn(\"pickup_datetime\", to_timestamp(col(\"pickup_datetime\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "    .withColumn(\"dropoff_datetime\", to_timestamp(col(\"dropoff_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "query2 = taxi_streaming_query2_df.writeStream \\\n",
    "    .trigger(once=True) \\\n",
    "    .foreachBatch(process_batch_query2) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "query2.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877f75ba-3043-4d11-a328-c2bb78baf5ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
